<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Algorithms · LinRegOutliers</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">LinRegOutliers</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../datasets/">Datasets</a></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../diagnostics/">Diagnostics</a></li><li class="is-active"><a class="tocitem" href>Algorithms</a><ul class="internal"><li><a class="tocitem" href="#Hadi-and-Simonoff-(1993)"><span>Hadi &amp; Simonoff (1993)</span></a></li><li><a class="tocitem" href="#Kianifard-and-Swallow-(1989)"><span>Kianifard &amp; Swallow (1989)</span></a></li><li><a class="tocitem" href="#Sebert-and-Montgomery-and-Rollier-(1998)"><span>Sebert &amp; Montgomery &amp; Rollier (1998)</span></a></li><li><a class="tocitem" href="#Least-Median-of-Squares"><span>Least Median of Squares</span></a></li><li><a class="tocitem" href="#Least-Trimmed-Squares"><span>Least Trimmed Squares</span></a></li><li><a class="tocitem" href="#Minimum-Volume-Ellipsoid-(MVE)"><span>Minimum Volume Ellipsoid (MVE)</span></a></li><li><a class="tocitem" href="#MVE-and-LTS-Plot"><span>MVE &amp; LTS Plot</span></a></li><li><a class="tocitem" href="#Billor-and-Chatterjee-and-Hadi-(2006)"><span>Billor &amp; Chatterjee &amp; Hadi (2006)</span></a></li><li><a class="tocitem" href="#Pena-and-Yohai-(1995)"><span>Pena &amp; Yohai (1995)</span></a></li><li><a class="tocitem" href="#Least-Trimmed-Absolute-Deviations-(LTA)"><span>Least Trimmed Absolute Deviations (LTA)</span></a></li><li><a class="tocitem" href="#Hadi-(1992)"><span>Hadi (1992)</span></a></li><li><a class="tocitem" href="#Marchette-and-Solka-(2003)-Data-Images"><span>Marchette &amp; Solka (2003) Data Images</span></a></li><li><a class="tocitem" href="#Satman&#39;s-GA-based-LTS-estimation-(2012)"><span>Satman&#39;s GA based LTS estimation (2012)</span></a></li><li><a class="tocitem" href="#Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm"><span>Barratt &amp; Angeris &amp; Boyd (2020) CCF algorithm</span></a></li><li><a class="tocitem" href="#Atkinson-(1994)-Forward-Search-Algorithm"><span>Atkinson (1994) Forward Search Algorithm</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Algorithms</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Algorithms</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jbytecode/LinRegOutliers/blob/master/docs/src/algorithms.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Algorithms"><a class="docs-heading-anchor" href="#Algorithms">Algorithms</a><a id="Algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithms" title="Permalink"></a></h1><h2 id="Hadi-and-Simonoff-(1993)"><a class="docs-heading-anchor" href="#Hadi-and-Simonoff-(1993)">Hadi &amp; Simonoff (1993)</a><a id="Hadi-and-Simonoff-(1993)-1"></a><a class="docs-heading-anchor-permalink" href="#Hadi-and-Simonoff-(1993)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hs93" href="#LinRegOutliers.hs93"><code>LinRegOutliers.hs93</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">hs93(setting; alpha = 0.05, basicsubsetindices = nothing)</code></pre><p>Perform the Hadi &amp; Simonoff (1993) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li><li><code>basicsubsetindices::Array{Int, 1}</code>: Initial basic subset, by default, the algorithm creates an initial set of clean observations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; hs93(reg0001)
Dict{Any,Any} with 3 entries:
  &quot;outliers&quot; =&gt; [14, 15, 16, 17, 18, 19, 20, 21]
  &quot;t&quot;        =&gt; -3.59263
  &quot;d&quot;        =&gt; [2.04474, 1.14495, -0.0633255, 0.0632934, -0.354349, -0.766818, -1.06862, -1.47638, -0.7…</code></pre><p><strong>References</strong></p><p>Hadi, Ali S., and Jeffrey S. Simonoff. &quot;Procedures for the identification of  multiple outliers in linear models.&quot; Journal of the American Statistical  Association 88.424 (1993): 1264-1272.</p></div></section></article><h2 id="Kianifard-and-Swallow-(1989)"><a class="docs-heading-anchor" href="#Kianifard-and-Swallow-(1989)">Kianifard &amp; Swallow (1989)</a><a id="Kianifard-and-Swallow-(1989)-1"></a><a class="docs-heading-anchor-permalink" href="#Kianifard-and-Swallow-(1989)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ks89" href="#LinRegOutliers.ks89"><code>LinRegOutliers.ks89</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ks89(setting; alpha = 0.05)</code></pre><p>Perform the Kianifard &amp; Swallow (1989) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; ks89(reg0001)
2-element Array{Int64,1}:
  4
 21</code></pre><p><strong>References</strong></p><p>Kianifard, Farid, and William H. Swallow. &quot;Using recursive residuals, calculated on adaptively-ordered observations, to identify outliers in linear regression.&quot; Biometrics (1989): 571-585.</p></div></section></article><h2 id="Sebert-and-Montgomery-and-Rollier-(1998)"><a class="docs-heading-anchor" href="#Sebert-and-Montgomery-and-Rollier-(1998)">Sebert &amp; Montgomery &amp; Rollier (1998)</a><a id="Sebert-and-Montgomery-and-Rollier-(1998)-1"></a><a class="docs-heading-anchor-permalink" href="#Sebert-and-Montgomery-and-Rollier-(1998)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.smr98" href="#LinRegOutliers.smr98"><code>LinRegOutliers.smr98</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">smr98(setting)</code></pre><p>Perform the Sebert, Monthomery and Rollier (1998) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; smr98(reg0001)
10-element Array{Int64,1}:
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24</code></pre><p><strong>References</strong></p><p>Sebert, David M., Douglas C. Montgomery, and Dwayne A. Rollier. &quot;A clustering algorithm for  identifying multiple outliers in linear regression.&quot; Computational statistics &amp; data analysis  27.4 (1998): 461-484.</p></div></section></article><h2 id="Least-Median-of-Squares"><a class="docs-heading-anchor" href="#Least-Median-of-Squares">Least Median of Squares</a><a id="Least-Median-of-Squares-1"></a><a class="docs-heading-anchor-permalink" href="#Least-Median-of-Squares" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lms" href="#LinRegOutliers.lms"><code>LinRegOutliers.lms</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">lms(setting; iters = nothing, crit = 2.5)</code></pre><p>Perform Least Median of Squares regression estimator with random sampling.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>iters::Int</code>: Number of random samples.</li><li><code>crit::Float64</code>: Critical value for standardized residuals. </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);

julia&gt; lms(reg)
Dict{Any,Any} with 6 entries:
  &quot;stdres&quot;    =&gt; [2.28328, 1.55551, 0.573308, 0.608843, 0.220321, -0.168202, -0.471913, -0.860435, -0.31603, -0.110871  …  85.7265, 88.9849, 103.269, 116.705, 135.229, 159.69,…
  &quot;S&quot;         =&gt; 1.17908
  &quot;outliers&quot;  =&gt; [14, 15, 16, 17, 18, 19, 20, 21]
  &quot;objective&quot; =&gt; 0.515348
  &quot;coef&quot;      =&gt; [-56.1972, 1.1581]
  &quot;crit&quot;      =&gt; 2.5</code></pre><p><strong>References</strong></p><p>Rousseeuw, Peter J. &quot;Least median of squares regression.&quot; Journal of the American  statistical association 79.388 (1984): 871-880.</p></div></section></article><h2 id="Least-Trimmed-Squares"><a class="docs-heading-anchor" href="#Least-Trimmed-Squares">Least Trimmed Squares</a><a id="Least-Trimmed-Squares-1"></a><a class="docs-heading-anchor-permalink" href="#Least-Trimmed-Squares" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lts" href="#LinRegOutliers.lts"><code>LinRegOutliers.lts</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">lts(setting; iters, crit)</code></pre><p>Perform the Fast-LTS (Least Trimmed Squares) algorithm for a given regression setting. </p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>iters::Int</code>: Number of iterations.</li><li><code>crit::Float64</code>: Critical value.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; lts(reg)
Dict{Any,Any} with 6 entries:
  &quot;betas&quot;            =&gt; [-56.5219, 1.16488]
  &quot;S&quot;                =&gt; 1.10918
  &quot;hsubset&quot;          =&gt; [11, 10, 5, 6, 23, 12, 13, 9, 24, 7, 3, 4, 8]
  &quot;outliers&quot;         =&gt; [14, 15, 16, 17, 18, 19, 20, 21]
  &quot;scaled.residuals&quot; =&gt; [2.41447, 1.63472, 0.584504, 0.61617, 0.197052, -0.222066, -0.551027, -0.970146, -0.397538, -0.185558  …  …
  &quot;objective&quot;        =&gt; 3.43133
</code></pre><p><strong>References</strong></p><p>Rousseeuw, Peter J., and Katrien Van Driessen. &quot;An algorithm for positive-breakdown  regression based on concentration steps.&quot; Data Analysis.  Springer, Berlin, Heidelberg, 2000. 335-346.</p></div></section></article><h2 id="Minimum-Volume-Ellipsoid-(MVE)"><a class="docs-heading-anchor" href="#Minimum-Volume-Ellipsoid-(MVE)">Minimum Volume Ellipsoid (MVE)</a><a id="Minimum-Volume-Ellipsoid-(MVE)-1"></a><a class="docs-heading-anchor-permalink" href="#Minimum-Volume-Ellipsoid-(MVE)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.mve" href="#LinRegOutliers.mve"><code>LinRegOutliers.mve</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">mve(data; alpha = 0.01)</code></pre><p>Performs the Minimum Volume Ellipsoid algorithm for a robust covariance matrix.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Multivariate data.</li><li><code>alpha::Float64</code>: Probability for quantiles of Chi-Squared statistic.</li></ul><p><strong>References</strong></p><p>Van Aelst, Stefan, and Peter Rousseeuw. &quot;Minimum volume ellipsoid.&quot; Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.</p></div></section></article><h2 id="MVE-and-LTS-Plot"><a class="docs-heading-anchor" href="#MVE-and-LTS-Plot">MVE &amp; LTS Plot</a><a id="MVE-and-LTS-Plot-1"></a><a class="docs-heading-anchor-permalink" href="#MVE-and-LTS-Plot" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.mveltsplot" href="#LinRegOutliers.mveltsplot"><code>LinRegOutliers.mveltsplot</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">mveltsplot(setting; alpha = 0.05, showplot = true)</code></pre><p>Generate MVE - LTS plot for visual detecting of regression outliers.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>alpha::Float64</code>: Probability for quantiles of Chi-Squared statistic.</li><li><code>showplot::Bool</code>: Whether a plot is shown or only return statistics.</li></ul><p><strong>References</strong></p><p>Van Aelst, Stefan, and Peter Rousseeuw. &quot;Minimum volume ellipsoid.&quot; Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.</p></div></section></article><h2 id="Billor-and-Chatterjee-and-Hadi-(2006)"><a class="docs-heading-anchor" href="#Billor-and-Chatterjee-and-Hadi-(2006)">Billor &amp; Chatterjee &amp; Hadi (2006)</a><a id="Billor-and-Chatterjee-and-Hadi-(2006)-1"></a><a class="docs-heading-anchor-permalink" href="#Billor-and-Chatterjee-and-Hadi-(2006)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.bch" href="#LinRegOutliers.bch"><code>LinRegOutliers.bch</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">bch(setting; alpha = 0.05, maxiter = 1000, epsilon = 0.000001)</code></pre><p>Perform the Billor &amp; Chatterjee &amp; Hadi (2006) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li><li><code>maxiter::Int</code>: Maximum number of iterations for calculating iterative weighted least squares estimates.</li><li><code>epsilon::Float64</code>: Accuracy for determining convergency.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg  = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; Dict{Any,Any} with 7 entries:
&quot;betas&quot;                               =&gt; [-55.9205, 1.15572]
&quot;squared.normalized.robust.distances&quot; =&gt; [0.104671, 0.0865052, 0.0700692, 0.0553633, 0.0423875, 0.03…
&quot;weights&quot;                             =&gt; [0.00186158, 0.00952088, 0.0787321, 0.0787321, 0.0787321, 0…
&quot;outliers&quot;                            =&gt; [1, 14, 15, 16, 17, 18, 19, 20, 21]
&quot;squared.normalized.residuals&quot;        =&gt; [5.53742e-5, 2.42977e-5, 2.36066e-6, 2.77706e-6, 1.07985e-7…
&quot;residuals&quot;                           =&gt; [2.5348, 1.67908, 0.523367, 0.567651, 0.111936, -0.343779, …
&quot;basic.subset&quot;                        =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  15, 16, 17, 18, 19, 20, …</code></pre><p><strong>References</strong></p><p>Billor, Nedret, Samprit Chatterjee, and Ali S. Hadi. &quot;A re-weighted least squares method  for robust regression estimation.&quot; American journal of mathematical and management sciences 26.3-4 (2006): 229-252.</p></div></section></article><h2 id="Pena-and-Yohai-(1995)"><a class="docs-heading-anchor" href="#Pena-and-Yohai-(1995)">Pena &amp; Yohai (1995)</a><a id="Pena-and-Yohai-(1995)-1"></a><a class="docs-heading-anchor-permalink" href="#Pena-and-Yohai-(1995)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.py95" href="#LinRegOutliers.py95"><code>LinRegOutliers.py95</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">py95(setting)</code></pre><p>Perform the Pena &amp; Yohai (1995) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);
julia&gt; py95(reg0001)
ict{Any,Any} with 2 entries:
  &quot;outliers&quot;       =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
  &quot;suspected.sets&quot; =&gt; Set([[14, 13], [43, 54, 24, 38, 22], [6, 10], [14, 7, 8, 3, 10, 2, 5, 6, 1, 9, 4…</code></pre><p><strong>References</strong></p><p>Peña, Daniel, and Victor J. Yohai. &quot;The detection of influential subsets in linear  regression by using an influence matrix.&quot; Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.</p></div></section></article><p>## Satman (2013)</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.satman2013" href="#LinRegOutliers.satman2013"><code>LinRegOutliers.satman2013</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">satman2013(setting)</code></pre><p>Perform Satman (2013) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);
julia&gt; satman2013(reg0001)
Dict{Any,Any} with 1 entry:
  &quot;outliers&quot; =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 47]
</code></pre><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p></div></section></article><p>## Satman (2015)</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.satman2015" href="#LinRegOutliers.satman2015"><code>LinRegOutliers.satman2015</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">satman2013(setting)</code></pre><p>Perform Satman (2015) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);
julia&gt; satman2013(reg0001)
Dict{Any,Any} with 1 entry:
  &quot;outliers&quot; =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 47]
</code></pre><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p></div></section></article><p>## Setan &amp; Halim &amp; Mohd (2000)</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.asm2000" href="#LinRegOutliers.asm2000"><code>LinRegOutliers.asm2000</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">asm2000(setting)</code></pre><p>Perform the Setan, Halim and Mohd (2000) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; asm2000(reg0001)
Dict{Any,Any} with 1 entry:
  &quot;outliers&quot; =&gt; [15, 16, 17, 18, 19, 20]</code></pre><p><strong>References</strong></p><p>Setan, Halim, and Mohd Nor Mohamad. &quot;Identifying multiple outliers in  linear regression: Robust fit and clustering approach.&quot; (2000).</p></div></section></article><p>## Least Absolute Deviations (LAD)</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lad" href="#LinRegOutliers.lad"><code>LinRegOutliers.lad</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">lad(setting; starting_betas = nothing)</code></pre><p>Perform Least Absolute Deviations regression for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>starting_betas::Array{Float64,1}</code>: Starting values of parameter estimations that fed to local search optimizer.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; lad(reg0001)
Dict{Any,Any} with 2 entries:
  &quot;betas&quot;     =&gt; [-57.3269, 1.19155]
  &quot;residuals&quot; =&gt; [2.14958, 1.25803, 0.0664872, 0.0749413, -0.416605, -0.90815, -1.2997, -1.79124,…
</code></pre></div></section><section><div><pre><code class="language-none">lad(X, y; starting_betas = nothing)</code></pre><p>Perform Least Absolute Deviations regression for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array{Float64, 2}</code>: Design matrix of the linear model.</li><li><code>y::Array{Float64, 1}</code>: Response vector of the linear model.</li><li><code>starting_betas::Array{Float64,1}</code>: Starting values of parameter estimations that fed to local search optimizer.</li></ul></div></section></article><h2 id="Least-Trimmed-Absolute-Deviations-(LTA)"><a class="docs-heading-anchor" href="#Least-Trimmed-Absolute-Deviations-(LTA)">Least Trimmed Absolute Deviations (LTA)</a><a id="Least-Trimmed-Absolute-Deviations-(LTA)-1"></a><a class="docs-heading-anchor-permalink" href="#Least-Trimmed-Absolute-Deviations-(LTA)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lta" href="#LinRegOutliers.lta"><code>LinRegOutliers.lta</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">lta(setting; exact = false)</code></pre><p>Perform the Hawkins &amp; Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>exact::Bool</code>: Consider all possible subsets of p or not where p is the number of regression parameters.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; lta(reg0001)
Dict{Any,Any} with 2 entries:
  &quot;betas&quot;     =&gt; [-55.5, 1.15]
  &quot;objective&quot; =&gt; 5.7

julia&gt; lta(reg0001, exact = true)
Dict{Any,Any} with 2 entries:
  &quot;betas&quot;     =&gt; [-55.5, 1.15]
  &quot;objective&quot; =&gt; 5.7  </code></pre><p><strong>References</strong></p><p>Hawkins, Douglas M., and David Olive. &quot;Applications and algorithms for least trimmed sum of  absolute deviations regression.&quot; Computational Statistics &amp; Data Analysis 32.2 (1999): 119-134.</p></div></section><section><div><pre><code class="language-none">lta(X, y; exact = false)</code></pre><p>Perform the Hawkins &amp; Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array{Float64, 2}</code>: Design matrix of linear regression model.</li><li><code>y::Array{Float64, 1}</code>: Response vector of linear regression model.</li><li><code>exact::Bool</code>: Consider all possible subsets of p or not where p is the number of regression parameters.</li></ul><p><strong>References</strong></p><p>Hawkins, Douglas M., and David Olive. &quot;Applications and algorithms for least trimmed sum of  absolute deviations regression.&quot; Computational Statistics &amp; Data Analysis 32.2 (1999): 119-134.</p></div></section></article><h2 id="Hadi-(1992)"><a class="docs-heading-anchor" href="#Hadi-(1992)">Hadi (1992)</a><a id="Hadi-(1992)-1"></a><a class="docs-heading-anchor-permalink" href="#Hadi-(1992)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hadi1992" href="#LinRegOutliers.hadi1992"><code>LinRegOutliers.hadi1992</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">hadi1992(multivariateData)</code></pre><p>Perform Hadi (1992) algorithm for a given multivariate data. </p><p># Arguments -<code>multivariate::Array{Float64, 2}</code>: Multivariate data.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; multidata = hcat(hbk.x1, hbk.x2, hbk.x3);

julia&gt; hadi1992(multidata)
Dict{Any,Any} with 3 entries:
  &quot;outliers&quot;              =&gt; [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
  &quot;criticial.chi.squared&quot; =&gt; 7.81473
  &quot;rth.robust.distance&quot;   =&gt; 5.04541</code></pre><p># Reference Hadi, Ali S. &quot;Identifying multiple outliers in multivariate data.&quot;  Journal of the Royal Statistical Society: Series B (Methodological) 54.3 (1992): 761-771.</p></div></section></article><h2 id="Marchette-and-Solka-(2003)-Data-Images"><a class="docs-heading-anchor" href="#Marchette-and-Solka-(2003)-Data-Images">Marchette &amp; Solka (2003) Data Images</a><a id="Marchette-and-Solka-(2003)-Data-Images-1"></a><a class="docs-heading-anchor-permalink" href="#Marchette-and-Solka-(2003)-Data-Images" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.dataimage" href="#LinRegOutliers.dataimage"><code>LinRegOutliers.dataimage</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">dataimage(dataMatrix; distance = :mahalanobis)</code></pre><p>Generate the Marchette &amp; Solka (2003) data image for a given data matrix. </p><p><strong>Arguments</strong></p><ul><li><code>dataMatrix::Array{Float64, 1}</code>: Data matrix with dimensions n x p, where n is the number of observations and p is the number of variables.</li><li><code>distance::Symbol</code>: Optional argument for the distance function.</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">distance is :mahalanobis by default, for the Mahalanobis distances. 
use 

    dataimage(mat, distance = :euclidean)

to use Euclidean distances.</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; x1 = hbk[:,&quot;x1&quot;];
julia&gt; x2 = hbk[:,&quot;x2&quot;];
julia&gt; x3 = hbk[:,&quot;x3&quot;];
julia&gt; mat = hcat(x1, x2, x3);
julia&gt; dataimage(mat)</code></pre><p><strong>References</strong></p><p>Marchette, David J., and Jeffrey L. Solka. &quot;Using data images for outlier detection.&quot;  Computational Statistics &amp; Data Analysis 43.4 (2003): 541-552.</p></div></section></article><h2 id="Satman&#39;s-GA-based-LTS-estimation-(2012)"><a class="docs-heading-anchor" href="#Satman&#39;s-GA-based-LTS-estimation-(2012)">Satman&#39;s GA based LTS estimation (2012)</a><a id="Satman&#39;s-GA-based-LTS-estimation-(2012)-1"></a><a class="docs-heading-anchor-permalink" href="#Satman&#39;s-GA-based-LTS-estimation-(2012)" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.galts" href="#LinRegOutliers.galts"><code>LinRegOutliers.galts</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">galts(setting)</code></pre><p>Perform Satman(2012) algorithm for estimating LTS coefficients.</p><p><strong>Arguments</strong></p><ul><li><code>setting</code>: A regression setting object.</li></ul><p><strong>Examples</strong></p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; galts(reg)
Dict{Any,Any} with 3 entries:
  &quot;betas&quot;       =&gt; [-56.5219, 1.16488]
  &quot;best.subset&quot; =&gt; [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 23, 24]
  &quot;objective&quot;   =&gt; 3.43133</code></pre><p><strong>References</strong></p><p>Satman, M. Hakan. &quot;A genetic algorithm based modification on the lts algorithm for large data sets.&quot;  Communications in Statistics-Simulation and Computation 41.5 (2012): 644-652.</p></div></section></article><p>## Fischler &amp; Bolles (1981) RANSAC Algorithm</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ransac" href="#LinRegOutliers.ransac"><code>LinRegOutliers.ransac</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ransac(setting; t, w=0.5, m=0, k=0, d=0, confidence=0.99)</code></pre><p>Run the RANSAC (1981) algorithm for the given regression setting</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and a dataset.</li><li><code>t::Float64</code>: The threshold distance of a sample point to the regression hyperplane to determine if it fits the model well.</li><li><code>w::Float64</code>: The probability of a sample point being inlier, default=0.5.</li><li><code>m::Int</code>: The number of points to sample to estimate the model parameter for each iteration. If set to 0, defaults to picking p points which is the minimum required.</li><li><code>k::Int</code>: The number of iterations to run. If set to 0, is calculated according to the formula given in the paper based on outlier probability and the sample set size.</li><li><code>d::Int</code>: The number of close data points required to accept the model. Defaults to number of data points multiplied by inlier ratio.</li><li><code>confidence::Float64</code>: Required to determine the number of optimum iterations if k is not specified.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; df = DataFrame(y=[0,1,2,3,3,4,10], x=[0,1,2,2,3,4,2])
julia&gt; reg = createRegressionSetting(@formula(y ~ x), df)
julia&gt; ransac(reg, t=0.8, w=0.85)
1-element Array{Int64,1}:
 7</code></pre><p><strong>References</strong></p><p>Martin A. Fischler &amp; Robert C. Bolles (June 1981). &quot;Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography&quot; Comm. ACM. 24 (6): 381–395.</p></div></section></article><p>## Minimum Covariance Determinant Estimator (MCD)</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.mcd" href="#LinRegOutliers.mcd"><code>LinRegOutliers.mcd</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">mcd(data; alpha = 0.01)</code></pre><p>Performs the Minimum Covariance Determinant algorithm for a robust covariance matrix.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Multivariate data.</li><li><code>alpha::Float64</code>: Probability for quantiles of Chi-Squared statistic.</li></ul><p># Notes Algorithm is implemented using concentration steps as described in the reference paper. However, details about number of iterations are slightly different.</p><p><strong>References</strong></p><p>Rousseeuw, Peter J., and Katrien Van Driessen. &quot;A fast algorithm for the minimum covariance  determinant estimator.&quot; Technometrics 41.3 (1999): 212-223.</p></div></section></article><p>## Imon (2005) Algorithm</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.imon2005" href="#LinRegOutliers.imon2005"><code>LinRegOutliers.imon2005</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">imon2005(setting)</code></pre><p>Perform the Imon 2005 algorithm for a given regression setting.</p><p># Arguments </p><ul><li><code>setting::RegressionSetting</code>: A regression setting.</li></ul><p><strong>Notes</strong></p><p>The implementation uses LTS rather than LMS as suggested in the paper. </p><p># Reference A. H. M. Rahmatullah Imon (2005) Identifying multiple influential observations in linear regression,  Journal of Applied Statistics, 32:9, 929-946, DOI: 10.1080/02664760500163599</p></div></section></article><h2 id="Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm"><a class="docs-heading-anchor" href="#Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm">Barratt &amp; Angeris &amp; Boyd (2020) CCF algorithm</a><a id="Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Barratt-and-Angeris-and-Boyd-(2020)-CCF-algorithm" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ccf" href="#LinRegOutliers.ccf"><code>LinRegOutliers.ccf</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">ccf(setting; starting_lambdas = nothing)</code></pre><p>Perform signed gradient descent for clipped convex functions for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>starting_lambdas::Array{Float64,1}</code>: Starting values of weighting parameters used by signed gradient descent.</li><li><code>alpha::Float64</code>: Loss at which a point is labeled as an outlier (points with loss ≥ alpha will be called outliers).</li><li><code>max_iter::Int64</code>: Maximum number of iterations to run signed gradient descent.</li><li><code>beta::Float64</code>: Step size parameter.</li><li><code>tol::Float64</code>: Tolerance below which convergence is declared.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; ccf(reg0001)
Dict{Any,Any} with 4 entries:
  &quot;betas&quot;     =&gt; [-63.4816, 1.30406]
  &quot;outliers&quot;  =&gt; [15, 16, 17, 18, 19, 20]
  &quot;lambdas&quot;   =&gt; [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  2.77556e-17, 2.77556e-17, 0…
  &quot;residuals&quot; =&gt; [-2.67878, -1.67473, -0.37067, -0.266613, 0.337444, 0.941501, 1.44556, 2.04962, 1…
</code></pre><p><strong>References</strong></p><p>Barratt, S., Angeris, G. &amp; Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4</p></div></section><section><div><pre><code class="language-none">ccf(X, y; starting_lambdas = nothing)</code></pre><p>Perform signed gradient descent for clipped convex functions for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array{Float64, 2}</code>: Design matrix of the linear model.</li><li><code>y::Array{Float64, 1}</code>: Response vector of the linear model.</li><li><code>starting_lambdas::Array{Float64,1}</code>: Starting values of weighting parameters used by signed gradient descent.</li><li><code>alpha::Float64</code>: Loss at which a point is labeled as an outlier. If unspecified, will be chosen as p*mean(residuals.^2), where residuals are OLS residuals.</li><li><code>p::Float64</code>: Points that have squared OLS residual greater than p times the mean squared OLS residual are considered outliers.</li><li><code>max_iter::Int64</code>: Maximum number of iterations to run signed gradient descent.</li><li><code>beta::Float64</code>: Step size parameter.</li><li><code>tol::Float64</code>: Tolerance below which convergence is declared.</li></ul><p><strong>References</strong></p><p>Barratt, S., Angeris, G. &amp; Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4</p></div></section></article><h2 id="Atkinson-(1994)-Forward-Search-Algorithm"><a class="docs-heading-anchor" href="#Atkinson-(1994)-Forward-Search-Algorithm">Atkinson (1994) Forward Search Algorithm</a><a id="Atkinson-(1994)-Forward-Search-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Atkinson-(1994)-Forward-Search-Algorithm" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.atkinson94" href="#LinRegOutliers.atkinson94"><code>LinRegOutliers.atkinson94</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">    atkinson94(setting, iters, crit)</code></pre><p>Runs the Atkinson94 algorithm to find out outliers using LMS method.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>iters::Int</code>: Number of random samples.</li><li><code>crit::Float64</code>: Critical value for residuals</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; atkinson94(reg)
Dict{Any,Any} with 6 entries:
  &quot;optimum_index&quot;    =&gt; 10
  &quot;residuals_matrix&quot; =&gt; [0.0286208 0.0620609 … 0.0796249 0.0; 0.0397778 0.120547 … 0.118437 0.0397778; … ; 1.21133 1.80846 … 0.690327 4.14366; 1.61977 0.971592 … 0.616204 3.58098]
  &quot;outliers&quot;         =&gt; [1, 3, 4, 21]
  &quot;objective&quot;        =&gt; 0.799134
  &quot;coef&quot;             =&gt; [-38.3133, 0.745659, 0.432794, 0.0104587]
  &quot;crit&quot;             =&gt; 3.0
</code></pre><p><strong>References</strong></p><p>Atkinson, Anthony C. &quot;Fast very robust methods for the detection of multiple outliers.&quot; Journal of the American Statistical Association 89.428 (1994): 1329-1339.</p></div></section></article><p>## BACON Algorithm (Billor &amp; Hadi &amp; Velleman (2000))</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.bacon" href="#LinRegOutliers.bacon"><code>LinRegOutliers.bacon</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">    bacon(setting, m, method, alpha)</code></pre><p>Run the BACON algorithm to detect outliers on regression data.</p><p><strong>Arguments:</strong></p><ul><li><code>setting</code>: RegressionSetting object with a formula and a dataset.</li><li><code>m</code>: The number of elements to be included in the initial subset.</li><li><code>method</code>: The distance method to use for selecting the points for initial subset</li><li><code>alpha</code>: The quantile used for cutoff</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; bacon_multivariate_outlier_detection(reg, m=12)
4-element Array{Int64,1}:
  1
  3
  4
 21
</code></pre><p><strong>References</strong></p><p>Billor, Nedret, Ali S. Hadi, and Paul F. Velleman. &quot;BACON: blocked adaptive computationally efficient outlier nominators.&quot; Computational statistics &amp; data analysis 34.3 (2000): 279-298.</p></div></section></article><p>## Hadi (1994) Algorithm</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hadi1994" href="#LinRegOutliers.hadi1994"><code>LinRegOutliers.hadi1994</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">hadi1994(multivariateData)</code></pre><p>Perform Hadi (1994) algorithm for a given multivariate data.</p><p># Arguments -<code>multivariate::Array{Float64, 2}</code>: Multivariate data.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; multidata = hcat(hbk.x1, hbk.x2, hbk.x3);

julia&gt; hadi1994(multidata)
Dict{Any,Any} with 3 entries:
  &quot;outliers&quot;              =&gt; [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
  &quot;critical.chi.squared&quot; =&gt; 7.81473
  &quot;rth.robust.distance&quot;   =&gt; 5.04541</code></pre><p># Reference Hadi, Ali S. &quot;A modification of a method for the dedection of outliers in multivariate samples&quot; Journal of the Royal Statistical Society: Series B (Methodological) 56.2 (1994): 393-396.</p></div></section></article><p>## Chatterjee &amp; Mächler (1997)</p><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.cm97" href="#LinRegOutliers.cm97"><code>LinRegOutliers.cm97</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">cm97(setting; maxiter = 1000)</code></pre><p>Perform the Chatterjee and Mächler (1997) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; myreg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; result = cm97(myreg)
Dict{String,Any} with 3 entries:
  &quot;betas&quot;      =&gt; [-37.0007, 0.839285, 0.632333, -0.113208]
  &quot;iterations&quot; =&gt; 22
  &quot;converged&quot;  =&gt; true</code></pre><p><strong>References</strong></p><p>Chatterjee, Samprit, and Martin Mächler. &quot;Robust regression: A weighted least squares approach.&quot;  Communications in Statistics-Theory and Methods 26.6 (1997): 1381-1394.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../diagnostics/">« Diagnostics</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 4 November 2020 17:17">Wednesday 4 November 2020</span>. Using Julia version 1.5.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
