<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>LinRegOutliers.jl Documentation · LinRegOutliers</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">LinRegOutliers</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>LinRegOutliers.jl Documentation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>LinRegOutliers.jl Documentation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>LinRegOutliers.jl Documentation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jbytecode/LinRegOutliers/blob/master/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="LinRegOutliers.jl-Documentation"><a class="docs-heading-anchor" href="#LinRegOutliers.jl-Documentation">LinRegOutliers.jl Documentation</a><a id="LinRegOutliers.jl-Documentation-1"></a><a class="docs-heading-anchor-permalink" href="#LinRegOutliers.jl-Documentation" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.RegressionSetting" href="#LinRegOutliers.RegressionSetting"><code>LinRegOutliers.RegressionSetting</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">struct RegressionSetting
    formula::FormulaTerm
    data::DataFrame
end

Immutable data structure for a regression setting.</code></pre><p><strong>Arguments</strong></p><ul><li><code>formula::FormulaTerm</code>: A formula object describes the linear regression model.</li><li><code>data::DataFrame</code>: DataFrame object holds the data</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">Implemented methods in this packages accepts linear models as RegressionSetting objects.
This objects holds the model formula and the data used in regression estimations.</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; setting = RegressionSetting(@formula(calls ~ year), phones)
RegressionSetting(calls ~ year, 24×2 DataFrame
│ Row │ year  │ calls   │
│     │ Int64 │ Float64 │
├─────┼───────┼─────────┤
│ 1   │ 50    │ 4.4     │
│ 2   │ 51    │ 4.7     │
│ 3   │ 52    │ 4.7     │
│ 4   │ 53    │ 5.9     │
│ 5   │ 54    │ 6.6     │
│ 6   │ 55    │ 7.3     │
│ 7   │ 56    │ 8.1     │
│ 8   │ 57    │ 8.8     │
│ 9   │ 58    │ 10.6    │
│ 10  │ 59    │ 12.0    │
⋮
│ 14  │ 63    │ 21.2    │
│ 15  │ 64    │ 119.0   │
│ 16  │ 65    │ 124.0   │
│ 17  │ 66    │ 142.0   │
│ 18  │ 67    │ 159.0   │
│ 19  │ 68    │ 182.0   │
│ 20  │ 69    │ 212.0   │
│ 21  │ 70    │ 43.0    │
│ 22  │ 71    │ 24.0    │
│ 23  │ 72    │ 27.0    │
│ 24  │ 73    │ 29.0    │)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.adjustedResiduals-Tuple{RegressionSetting}" href="#LinRegOutliers.adjustedResiduals-Tuple{RegressionSetting}"><code>LinRegOutliers.adjustedResiduals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">adjustedResiduals(setting)</code></pre><p>Calculate adjusted residuals for a given regression setting.</p><p># Arguments:</p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; adjustedResiduals(reg)
24-element Array{Float64,1}:
  13.486773572526268
   8.2307993473897
   2.774371467851612
  -1.3093999279776498
  -5.851901346871404
 -10.335509559699863
 -14.670907823058053
 -19.07911256736661
 -22.338710565623828
 -26.00786250934617
 -29.58187157605512
 -33.27523207616458
 -37.19977737822219
 -37.173743587631165
  57.781855070799956
  57.898085871534626
  71.47231139524963
  84.19185329435882
 103.37399662263209
 130.23557965295348
 -52.6720662600165
 -78.76891816539992
 -81.75736547266746
 -85.9914301855088</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.applyColumns-Tuple{Function,Array{T,2} where T}" href="#LinRegOutliers.applyColumns-Tuple{Function,Array{T,2} where T}"><code>LinRegOutliers.applyColumns</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">applyColumns(f, data)

Apply function f to each columns of data.</code></pre><p><strong>Arguments</strong></p><ul><li><code>f::Function</code>: A function that takes a one dimensional array as argument.</li><li><code>data::Matrix</code>: A Matrix object.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.applyColumns-Tuple{Function,DataFrame}" href="#LinRegOutliers.applyColumns-Tuple{Function,DataFrame}"><code>LinRegOutliers.applyColumns</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">applyColumns(f, data)

Apply function f to each columns of data.</code></pre><p><strong>Arguments</strong></p><ul><li><code>f::Function</code>: A function that takes a one dimensional array as argument.</li><li><code>data::DataFrame</code>: A DataFrame object.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.asm2000-Tuple{RegressionSetting}" href="#LinRegOutliers.asm2000-Tuple{RegressionSetting}"><code>LinRegOutliers.asm2000</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">asm2000(setting)</code></pre><p>Perform the Setan, Halim and Mohd (2000) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; asm2000(reg0001)
Dict{Any,Any} with 1 entry:
  &quot;outliers&quot; =&gt; [15, 16, 17, 18, 19, 20]</code></pre><p><strong>References</strong></p><p>Setan, Halim, and Mohd Nor Mohamad. &quot;Identifying multiple outliers in  linear regression: Robust fit and clustering approach.&quot; (2000).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.atkinson94-Tuple{RegressionSetting}" href="#LinRegOutliers.atkinson94-Tuple{RegressionSetting}"><code>LinRegOutliers.atkinson94</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">    atkinson94(setting, iters, crit)</code></pre><p>Runs the Atkinson94 algorithm to find out outliers using LMS method.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>iters::Int</code>: Number of random samples.</li><li><code>crit::Float64</code>: Critical value for residuals</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; atkinson94(reg)
Dict{Any,Any} with 6 entries:
  &quot;optimum_index&quot;    =&gt; 10
  &quot;residuals_matrix&quot; =&gt; [0.0286208 0.0620609 … 0.0796249 0.0; 0.0397778 0.120547 … 0.118437 0.0397778; … ; 1.21133 1.80846 … 0.690327 4.14366; 1.61977 0.971592 … 0.616204 3.58098]
  &quot;outliers&quot;         =&gt; [1, 3, 4, 21]
  &quot;objective&quot;        =&gt; 0.799134
  &quot;coef&quot;             =&gt; [-38.3133, 0.745659, 0.432794, 0.0104587]
  &quot;crit&quot;             =&gt; 3.0
</code></pre><p><strong>References</strong></p><p>Atkinson, Anthony C. &quot;Fast very robust methods for the detection of multiple outliers.&quot; Journal of the American Statistical Association 89.428 (1994): 1329-1339.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.atkinsonstalactiteplot-Tuple{RegressionSetting}" href="#LinRegOutliers.atkinsonstalactiteplot-Tuple{RegressionSetting}"><code>LinRegOutliers.atkinsonstalactiteplot</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Runs the atkinson94 algorithm and additionally plots the Stalactite text plot as described in the paper. Works for data with less than 100 points at the moment (since screen width of 80-120 is the standard).</p><p><strong>References</strong></p><p>Atkinson, Anthony C. &quot;Fast very robust methods for the detection of multiple outliers.&quot; Journal of the American Statistical Association 89.428 (1994): 1329-1339.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.bacon-Tuple{RegressionSetting}" href="#LinRegOutliers.bacon-Tuple{RegressionSetting}"><code>LinRegOutliers.bacon</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">    bacon(setting, m, method, alpha)</code></pre><p>Run the BACON algorithm to detect outliers on regression data.</p><p><strong>Arguments:</strong></p><ul><li><code>setting</code>: RegressionSetting object with a formula and a dataset.</li><li><code>m</code>: The number of elements to be included in the initial subset.</li><li><code>method</code>: The distance method to use for selecting the points for initial subset</li><li><code>alpha</code>: The quantile used for cutoff</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; bacon_multivariate_outlier_detection(reg, m=12)
4-element Array{Int64,1}:
  1
  3
  4
 21
</code></pre><p><strong>References</strong></p><p>Billor, Nedret, Ali S. Hadi, and Paul F. Velleman. &quot;BACON: blocked adaptive computationally efficient outlier nominators.&quot; Computational statistics &amp; data analysis 34.3 (2000): 279-298.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.bacon_multivariate_outlier_detection-Tuple{Array{Float64,2},Int64}" href="#LinRegOutliers.bacon_multivariate_outlier_detection-Tuple{Array{Float64,2},Int64}"><code>LinRegOutliers.bacon_multivariate_outlier_detection</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">    bacon_multivariate_outlier_detection(X, m, method, alpha)</code></pre><p>This function performs the outlier detection for multivariate data according to algorithm #3. This is used by algorithm #4 to compute the initial subset.</p><p><strong>Arguments</strong></p><ul><li><code>X</code>: The multivariate data matrix.</li><li><code>m</code>: The minimum number of points to include in the initial subset</li><li><code>method</code>: The distance method to use for selecting the points for initial subset</li><li><code>alpha</code>: The quantile used for cutoff</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.bacon_regression_initial_subset-Tuple{Array{Float64,2},Array{Float64,N} where N,Int64}" href="#LinRegOutliers.bacon_regression_initial_subset-Tuple{Array{Float64,2},Array{Float64,N} where N,Int64}"><code>LinRegOutliers.bacon_regression_initial_subset</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">    bacon_regression_initial_subset(X, y, m, method, alpha)</code></pre><p>This function computes the initial subset having at least m elements which are likely to be free of outliers used for the BACON algorithm.</p><p><strong>Arguments:</strong></p><ul><li><code>X</code>: The multivariate data matrix.</li><li><code>y</code>: The response vector.</li><li><code>m</code>: The minimum number of points to include in the initial subset</li><li><code>method</code>: The distance method to use for selecting the points for initial subset</li><li><code>alpha</code>: The quantile used for cutoff</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.bch-Tuple{RegressionSetting}" href="#LinRegOutliers.bch-Tuple{RegressionSetting}"><code>LinRegOutliers.bch</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">bch(setting; alpha = 0.05, maxiter = 1000, epsilon = 0.000001)</code></pre><p>Perform the Billor &amp; Chatterjee &amp; Hadi (2006) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li><li><code>maxiter::Int</code>: Maximum number of iterations for calculating iterative weighted least squares estimates.</li><li><code>epsilon::Float64</code>: Accuracy for determining convergency.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg  = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; Dict{Any,Any} with 7 entries:
&quot;betas&quot;                               =&gt; [-55.9205, 1.15572]
&quot;squared.normalized.robust.distances&quot; =&gt; [0.104671, 0.0865052, 0.0700692, 0.0553633, 0.0423875, 0.03…
&quot;weights&quot;                             =&gt; [0.00186158, 0.00952088, 0.0787321, 0.0787321, 0.0787321, 0…
&quot;outliers&quot;                            =&gt; [1, 14, 15, 16, 17, 18, 19, 20, 21]
&quot;squared.normalized.residuals&quot;        =&gt; [5.53742e-5, 2.42977e-5, 2.36066e-6, 2.77706e-6, 1.07985e-7…
&quot;residuals&quot;                           =&gt; [2.5348, 1.67908, 0.523367, 0.567651, 0.111936, -0.343779, …
&quot;basic.subset&quot;                        =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  15, 16, 17, 18, 19, 20, …</code></pre><p><strong>References</strong></p><p>Billor, Nedret, Samprit Chatterjee, and Ali S. Hadi. &quot;A re-weighted least squares method  for robust regression estimation.&quot; American journal of mathematical and management sciences 26.3-4 (2006): 229-252.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.bchplot-Tuple{RegressionSetting}" href="#LinRegOutliers.bchplot-Tuple{RegressionSetting}"><code>LinRegOutliers.bchplot</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">bchplot(setting::RegressionSetting; alpha=0.05, maxiter=1000, epsilon=0.00001)</code></pre><p>Perform the Billor &amp; Chatterjee &amp; Hadi (2006) algorithm and generates outlier plot  for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li><li><code>maxiter::Int</code>: Maximum number of iterations for calculating iterative weighted least squares estimates.</li><li><code>epsilon::Float64</code>: Accuracy for determining convergency.</li></ul><p><strong>References</strong></p><p>Billor, Nedret, Samprit Chatterjee, and Ali S. Hadi. &quot;A re-weighted least squares method  for robust regression estimation.&quot; American journal of mathematical and management sciences 26.3-4 (2006): 229-252.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ccf-Tuple{Array{Float64,2},Array{Float64,1}}" href="#LinRegOutliers.ccf-Tuple{Array{Float64,2},Array{Float64,1}}"><code>LinRegOutliers.ccf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ccf(X, y; starting_lambdas = nothing)</code></pre><p>Perform signed gradient descent for clipped convex functions for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array{Float64, 2}</code>: Design matrix of the linear model.</li><li><code>y::Array{Float64, 1}</code>: Response vector of the linear model.</li><li><code>starting_lambdas::Array{Float64,1}</code>: Starting values of weighting parameters used by signed gradient descent.</li><li><code>alpha::Float64</code>: Loss at which a point is labeled as an outlier. If unspecified, will be chosen as p*mean(residuals.^2), where residuals are OLS residuals.</li><li><code>p::Float64</code>: Points that have squared OLS residual greater than p times the mean squared OLS residual are considered outliers.</li><li><code>max_iter::Int64</code>: Maximum number of iterations to run signed gradient descent.</li><li><code>beta::Float64</code>: Step size parameter.</li><li><code>tol::Float64</code>: Tolerance below which convergence is declared.</li></ul><p><strong>References</strong></p><p>Barratt, S., Angeris, G. &amp; Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ccf-Tuple{RegressionSetting}" href="#LinRegOutliers.ccf-Tuple{RegressionSetting}"><code>LinRegOutliers.ccf</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ccf(setting; starting_lambdas = nothing)</code></pre><p>Perform signed gradient descent for clipped convex functions for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>starting_lambdas::Array{Float64,1}</code>: Starting values of weighting parameters used by signed gradient descent.</li><li><code>alpha::Float64</code>: Loss at which a point is labeled as an outlier (points with loss ≥ alpha will be called outliers).</li><li><code>max_iter::Int64</code>: Maximum number of iterations to run signed gradient descent.</li><li><code>beta::Float64</code>: Step size parameter.</li><li><code>tol::Float64</code>: Tolerance below which convergence is declared.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; ccf(reg0001)
Dict{Any,Any} with 4 entries:
  &quot;betas&quot;     =&gt; [-63.4816, 1.30406]
  &quot;outliers&quot;  =&gt; [15, 16, 17, 18, 19, 20]
  &quot;lambdas&quot;   =&gt; [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  2.77556e-17, 2.77556e-17, 0…
  &quot;residuals&quot; =&gt; [-2.67878, -1.67473, -0.37067, -0.266613, 0.337444, 0.941501, 1.44556, 2.04962, 1…
</code></pre><p><strong>References</strong></p><p>Barratt, S., Angeris, G. &amp; Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.cga-Tuple{}" href="#LinRegOutliers.cga-Tuple{}"><code>LinRegOutliers.cga</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Performs a CGA (Compact Genetic Algorithm) search for minimization of an objective function. In the example below, the objective function is to minimize sum of bits of a binary vector. The search method results the optimum vector of [0, 0, ..., 0] where the objective function is zero.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; function f(x)
           return sum(x)
       end
f (generic function with 1 method)
julia&gt; cga(chsize = 10, costfunction = f, popsize = 100)
10-element Array{Bool,1}:
 0
 0
 0
 0
 0
 0
 0
 0
 0
 0</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.cgasample-Tuple{Array{Float64,1}}" href="#LinRegOutliers.cgasample-Tuple{Array{Float64,1}}"><code>LinRegOutliers.cgasample</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Generates a binary array of values using a probability vector. Each single element of the probability vector is the probability of bit having  the value of 1. When the probability vector is [1, 1, 1, ..., 1] then the sampled vector is [1.0, 1.0, 1.0, ..., 1.0] whereas it is [0.0, 0.0, 0.0, ..., 0.0] when the probability vector is a vector of zeros. The CGA (compact genetic algorithms) search is started using the  probability vector of [0.5, 0.5, 0.5, ..., 0.5] which produces random vectors of either zeros or ones.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; sample([1, 1, 1, 1, 1])
5-element Array{Bool,1}:
 1
 1
 1
 1
 1
julia&gt; cgasample(ones(10) * 0.5)
10-element Array{Bool,1}:
 1
 1
 1
 1
 0
 0
 0
 1
 1
 0</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.cm97-Tuple{RegressionSetting}" href="#LinRegOutliers.cm97-Tuple{RegressionSetting}"><code>LinRegOutliers.cm97</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cm97(setting; maxiter = 1000)</code></pre><p>Perform the Chatterjee and Mächler (1997) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; myreg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; result = cm97(myreg)
Dict{String,Any} with 3 entries:
  &quot;betas&quot;      =&gt; [-37.0007, 0.839285, 0.632333, -0.113208]
  &quot;iterations&quot; =&gt; 22
  &quot;converged&quot;  =&gt; true</code></pre><p><strong>References</strong></p><p>Chatterjee, Samprit, and Martin Mächler. &quot;Robust regression: A weighted least squares approach.&quot;  Communications in Statistics-Theory and Methods 26.6 (1997): 1381-1394.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.compute_t_distance-Tuple{Array{Float64,2},Array{Float64,N} where N,Array{Int64,N} where N}" href="#LinRegOutliers.compute_t_distance-Tuple{Array{Float64,2},Array{Float64,N} where N,Array{Int64,N} where N}"><code>LinRegOutliers.compute_t_distance</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">    compute_t_distance(X, y, subset)</code></pre><p>This function computes the t distance for each point and returns the distance vector.</p><p><strong>Arguments:</strong></p><ul><li><code>X</code>: The multivariate data matrix.</li><li><code>y</code>: The output vector</li><li><code>subset</code>: The vector which denotes the points inside the subset, used to scale the residuals accordingly.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.cooks-Tuple{RegressionSetting}" href="#LinRegOutliers.cooks-Tuple{RegressionSetting}"><code>LinRegOutliers.cooks</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">cooks(setting)</code></pre><p>Calculate Cook distances for all observations in a regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);

julia&gt; cooks(reg)
24-element Array{Float64,1}:
 0.005344774190779822
 0.0017088194691033689
 0.00016624914057962608
 3.1644452583114795e-5
 0.0005395058666404081
 0.0014375008774859539
 0.0024828140956511258
 0.0036279720445167277
 0.004357605989540906
 0.005288503758364767
 0.006313578057565415
 0.0076561205696857254
 0.009568574875389256
 0.009970039008782357
 0.02610396373381051
 0.029272523880917646
 0.05091236198400663
 0.08176555044049343
 0.14380266904640235
 0.26721539425047447
 0.051205153558783356
 0.13401084683481085
 0.16860324592350226
 0.2172819114905912</code></pre><p><strong>References</strong></p><p>Cook, R. Dennis. &quot;Detection of influential observation in linear regression.&quot;  Technometrics 19.1 (1977): 15-18.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.coordinatwisemedians-Tuple{Array{Float64,2}}" href="#LinRegOutliers.coordinatwisemedians-Tuple{Array{Float64,2}}"><code>LinRegOutliers.coordinatwisemedians</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">coordinatwisemedians(datamat)

Return vector of medians of each column in a matrix.</code></pre><p><strong>Arguments</strong></p><ul><li><code>datamat::Array{Float64, 2}</code>: A matrix.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; mat = [1.0 2.0; 3.0 4.0; 5.0 6.0]
3×2 Array{Float64,2}:
 1.0  2.0
 3.0  4.0
 5.0  6.0

julia&gt; coordinatwisemedians(mat)
2-element Array{Float64,1}:
 3.0
 4.0</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.covratio-Tuple{RegressionSetting,Int64}" href="#LinRegOutliers.covratio-Tuple{RegressionSetting,Int64}"><code>LinRegOutliers.covratio</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">covratio(setting, omittedIndex)</code></pre><p>Apply covariance ratio diagnostic for a given regression setting and observation index.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>omittedIndex::Int</code>: Index of the omitted observation.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; setting = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; covratio(setting, 1)
1.2945913799871505</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.createRegressionSetting-Tuple{StatsModels.FormulaTerm,DataFrame}" href="#LinRegOutliers.createRegressionSetting-Tuple{StatsModels.FormulaTerm,DataFrame}"><code>LinRegOutliers.createRegressionSetting</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">createRegressionSetting(formula, data)

Create a regression setting for a given formula and data</code></pre><p><strong>Arguments</strong></p><ul><li><code>formula::FormulaTerm</code>: A formula object describes the linear regression model.</li><li><code>data::DataFrame</code>: DataFrame object holds the data</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">Implemented methods in this packages accepts linear models as RegressionSetting objects.
This objects holds the model formula and the data used in regression estimations.
createRegressionSetting is a helper function for creating RegressionSetting objects.</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; setting = createRegressionSetting(@formula(calls ~ year), phones)
RegressionSetting(calls ~ year, 24×2 DataFrame
│ Row │ year  │ calls   │
│     │ Int64 │ Float64 │
├─────┼───────┼─────────┤
│ 1   │ 50    │ 4.4     │
│ 2   │ 51    │ 4.7     │
│ 3   │ 52    │ 4.7     │
│ 4   │ 53    │ 5.9     │
│ 5   │ 54    │ 6.6     │
│ 6   │ 55    │ 7.3     │
│ 7   │ 56    │ 8.1     │
│ 8   │ 57    │ 8.8     │
│ 9   │ 58    │ 10.6    │
│ 10  │ 59    │ 12.0    │
⋮
│ 14  │ 63    │ 21.2    │
│ 15  │ 64    │ 119.0   │
│ 16  │ 65    │ 124.0   │
│ 17  │ 66    │ 142.0   │
│ 18  │ 67    │ 159.0   │
│ 19  │ 68    │ 182.0   │
│ 20  │ 69    │ 212.0   │
│ 21  │ 70    │ 43.0    │
│ 22  │ 71    │ 24.0    │
│ 23  │ 72    │ 27.0    │
│ 24  │ 73    │ 29.0    │)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.dataimage-Tuple{Array{Float64,2}}" href="#LinRegOutliers.dataimage-Tuple{Array{Float64,2}}"><code>LinRegOutliers.dataimage</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">dataimage(dataMatrix; distance = :mahalanobis)</code></pre><p>Generate the Marchette &amp; Solka (2003) data image for a given data matrix. </p><p><strong>Arguments</strong></p><ul><li><code>dataMatrix::Array{Float64, 1}</code>: Data matrix with dimensions n x p, where n is the number of observations and p is the number of variables.</li><li><code>distance::Symbol</code>: Optional argument for the distance function.</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">distance is :mahalanobis by default, for the Mahalanobis distances. 
use 

    dataimage(mat, distance = :euclidean)

to use Euclidean distances.</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; x1 = hbk[:,&quot;x1&quot;];
julia&gt; x2 = hbk[:,&quot;x2&quot;];
julia&gt; x3 = hbk[:,&quot;x3&quot;];
julia&gt; mat = hcat(x1, x2, x3);
julia&gt; dataimage(mat)</code></pre><p><strong>References</strong></p><p>Marchette, David J., and Jeffrey L. Solka. &quot;Using data images for outlier detection.&quot;  Computational Statistics &amp; Data Analysis 43.4 (2003): 541-552.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.designMatrix-Tuple{RegressionSetting}" href="#LinRegOutliers.designMatrix-Tuple{RegressionSetting}"><code>LinRegOutliers.designMatrix</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">designMatrix(setting)

Return matrix of independent variables including the variable (ones) of the constanst term for a given regression setting.</code></pre><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">Design matrix is a matrix which holds values of independent variables on its columns for a given linear regression model.</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; setting = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; designMatrix(setting)
24×2 Array{Float64,2}:
 1.0  50.0
 1.0  51.0
 1.0  52.0
 1.0  53.0
 1.0  54.0
 1.0  55.0
 1.0  56.0
 1.0  57.0
 1.0  58.0
 1.0  59.0
 1.0  60.0
 1.0  61.0
 1.0  62.0
 1.0  63.0
 1.0  64.0
 1.0  65.0
 1.0  66.0
 1.0  67.0
 1.0  68.0
 1.0  69.0
 1.0  70.0
 1.0  71.0
 1.0  72.0
 1.0  73.0</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.dfbeta-Tuple{RegressionSetting,Int64}" href="#LinRegOutliers.dfbeta-Tuple{RegressionSetting,Int64}"><code>LinRegOutliers.dfbeta</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">dfbeta(setting, omittedIndex)</code></pre><p>Apply DFBETA diagnostic for a given regression setting and observation index.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>omittedIndex::Int</code>: Index of the omitted observation.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; setting = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; dfbeta(setting, 1)
2-element Array{Float64,1}:
  9.643915678524024
 -0.14686166007904422</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.dffit-Tuple{RegressionSetting,Int64}" href="#LinRegOutliers.dffit-Tuple{RegressionSetting,Int64}"><code>LinRegOutliers.dffit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">dffit(setting, i)</code></pre><p>Calculate the effect of the ith observation on the linear regression fit.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>i::Int</code>: Index of the observation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; dffit(reg, 1)
2.3008326745719785

julia&gt; dffit(reg, 15)
2.7880619386124295

julia&gt; dffit(reg, 16)
3.1116532421969794

julia&gt; dffit(reg, 17)
4.367981450347031

julia&gt; dffit(reg, 21)
-5.81610150322166</code></pre><p><strong>References</strong></p><p>Belsley, David A., Edwin Kuh, and Roy E. Welsch. Regression diagnostics:  Identifying influential data and sources of collinearity. Vol. 571. John Wiley &amp; Sons, 2005.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.dffit-Tuple{RegressionSetting}" href="#LinRegOutliers.dffit-Tuple{RegressionSetting}"><code>LinRegOutliers.dffit</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">dffit(setting)</code></pre><p>Calculate <code>dffit</code> for all observations.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);

julia&gt; dffit(reg)
24-element Array{Float64,1}:
   2.3008326745719785
   1.2189579001467337
   0.35535667547543426
  -0.14458523141740898
  -0.5558346324846752
  -0.8441316814464983
  -1.0329184407957257
  -1.16600692151232
  -1.2005633711667656
  -1.2549187193476428
  -1.3195581500053777
  -1.42383876236147
  -1.5917690629803474
  -1.6582086833534504
   2.7880619386124295
   3.1116532421969794
   4.367981450347031
   5.927603041427858
   8.442860517217582
  12.370243663029527
  -5.81610150322166
 -10.089153963127842
 -12.10803256546825
 -14.67006851119936</code></pre><p><strong>References</strong></p><p>Belsley, David A., Edwin Kuh, and Roy E. Welsch. Regression diagnostics:  Identifying influential data and sources of collinearity. Vol. 571. John Wiley &amp; Sons, 2005.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.dominates-Tuple{Array,Array}" href="#LinRegOutliers.dominates-Tuple{Array,Array}"><code>LinRegOutliers.dominates</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">dominates(p1::Array, p2::Array)</code></pre><p>Return true if each element in p1 is not less than the corresponding element in p2 and at least one element in p1 is bigger than the corresponding element in p2.</p><p><strong>Arguments</strong></p><ul><li><code>p1::Array</code>: Numeric array of n elements.</li><li><code>p2::Array</code>: Numeric array of n elements.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; dominates([1,2,3], [1,2,1])
true

julia&gt; dominates([0,0,0,0], [1,0,0,0])
false</code></pre><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p><p>Deb, Kalyanmoy, et al. &quot;A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II.&quot;  International conference on parallel problem solving from nature. Springer, Berlin, Heidelberg, 2000.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.euclideanDistances-Tuple{Array{Float64,2}}" href="#LinRegOutliers.euclideanDistances-Tuple{Array{Float64,2}}"><code>LinRegOutliers.euclideanDistances</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">euclideanDistances(dataMatrix)</code></pre><p>Calculate Euclidean distances between pairs. </p><p><strong>Arguments</strong></p><ul><li><code>dataMatrix::Array{Float64, 1}</code>: Data matrix with dimensions n x p, where n is the number of observations and p is the number of variables.</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">This is the helper function for the dataimage() function defined in Marchette &amp; Solka (2003).</code></pre><p><strong>References</strong></p><p>Marchette, David J., and Jeffrey L. Solka. &quot;Using data images for outlier detection.&quot;  Computational Statistics &amp; Data Analysis 43.4 (2003): 541-552.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.find_minimum_nonzero-Tuple{Array{Float64,1}}" href="#LinRegOutliers.find_minimum_nonzero-Tuple{Array{Float64,1}}"><code>LinRegOutliers.find_minimum_nonzero</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">find_minimum_nonzero(arr)

Return minimum of numbers greater than zero.</code></pre><p><strong>Arguments</strong></p><ul><li><code>arr::Array{Float64, 1}</code>: A function that takes a one dimensional array as argument.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; find_minimum_nonzero([0.0, 0.0, 5.0, 1.0])
1.0</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.galts-Tuple{RegressionSetting}" href="#LinRegOutliers.galts-Tuple{RegressionSetting}"><code>LinRegOutliers.galts</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">galts(setting)</code></pre><p>Perform Satman(2012) algorithm for estimating LTS coefficients.</p><p><strong>Arguments</strong></p><ul><li><code>setting</code>: A regression setting object.</li></ul><p><strong>Examples</strong></p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; galts(reg)
Dict{Any,Any} with 3 entries:
  &quot;betas&quot;       =&gt; [-56.5219, 1.16488]
  &quot;best.subset&quot; =&gt; [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 23, 24]
  &quot;objective&quot;   =&gt; 3.43133</code></pre><p><strong>References</strong></p><p>Satman, M. Hakan. &quot;A genetic algorithm based modification on the lts algorithm for large data sets.&quot;  Communications in Statistics-Simulation and Computation 41.5 (2012): 644-652.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hadi1992-Tuple{Array{Float64,2}}" href="#LinRegOutliers.hadi1992-Tuple{Array{Float64,2}}"><code>LinRegOutliers.hadi1992</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">hadi1992(multivariateData)</code></pre><p>Perform Hadi (1992) algorithm for a given multivariate data. </p><p># Arguments -<code>multivariate::Array{Float64, 2}</code>: Multivariate data.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; multidata = hcat(hbk.x1, hbk.x2, hbk.x3);

julia&gt; hadi1992(multidata)
Dict{Any,Any} with 3 entries:
  &quot;outliers&quot;              =&gt; [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
  &quot;criticial.chi.squared&quot; =&gt; 7.81473
  &quot;rth.robust.distance&quot;   =&gt; 5.04541</code></pre><p># Reference Hadi, Ali S. &quot;Identifying multiple outliers in multivariate data.&quot;  Journal of the Royal Statistical Society: Series B (Methodological) 54.3 (1992): 761-771.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hadi1992_handle_singularity-Tuple{Array{Float64,2}}" href="#LinRegOutliers.hadi1992_handle_singularity-Tuple{Array{Float64,2}}"><code>LinRegOutliers.hadi1992_handle_singularity</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">hadi1992_handle_singularity(S)</code></pre><p>Perform the sub-algorithm of handling singularity defined in Hadi (1992).</p><p># Arguments </p><ul><li><code>S::Array{Float64, 2}</code>: A covariance matrix.</li></ul><p># Reference Hadi, Ali S. &quot;Identifying multiple outliers in multivariate data.&quot;  Journal of the Royal Statistical Society: Series B (Methodological) 54.3 (1992): 761-771.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hadi1994-Tuple{Array{Float64,2}}" href="#LinRegOutliers.hadi1994-Tuple{Array{Float64,2}}"><code>LinRegOutliers.hadi1994</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">hadi1994(multivariateData)</code></pre><p>Perform Hadi (1994) algorithm for a given multivariate data.</p><p># Arguments -<code>multivariate::Array{Float64, 2}</code>: Multivariate data.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; multidata = hcat(hbk.x1, hbk.x2, hbk.x3);

julia&gt; hadi1994(multidata)
Dict{Any,Any} with 3 entries:
  &quot;outliers&quot;              =&gt; [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
  &quot;critical.chi.squared&quot; =&gt; 7.81473
  &quot;rth.robust.distance&quot;   =&gt; 5.04541</code></pre><p># Reference Hadi, Ali S. &quot;A modification of a method for the dedection of outliers in multivariate samples&quot; Journal of the Royal Statistical Society: Series B (Methodological) 56.2 (1994): 393-396.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hadimeasure-Tuple{RegressionSetting}" href="#LinRegOutliers.hadimeasure-Tuple{RegressionSetting}"><code>LinRegOutliers.hadimeasure</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">hadimeasure(setting; c = 2.0)</code></pre><p>Apply Hadi&#39;s regression diagnostic for a given regression setting</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>c::Float64</code>: Critical value selected between 2.0 - 3.0. The default is 2.0.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; setting = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; hadimeasure(setting)</code></pre><p><strong>References</strong></p><p>Chatterjee, Samprit and Hadi, Ali. Regression Analysis by Example.      5th ed. N.p.: John Wiley &amp; Sons, 2012.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hatmatrix-Tuple{RegressionSetting}" href="#LinRegOutliers.hatmatrix-Tuple{RegressionSetting}"><code>LinRegOutliers.hatmatrix</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">hatmatrix(setting)</code></pre><p>Calculate Hat matrix of dimensions n x n for a given regression setting with n observations.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li></ul><p><strong>Examples</strong></p><p>```julia-repl julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones); julia&gt; size(hatmatrix(reg))</p><p>(24, 24)</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hs93-Tuple{RegressionSetting}" href="#LinRegOutliers.hs93-Tuple{RegressionSetting}"><code>LinRegOutliers.hs93</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">hs93(setting; alpha = 0.05, basicsubsetindices = nothing)</code></pre><p>Perform the Hadi &amp; Simonoff (1993) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li><li><code>basicsubsetindices::Array{Int, 1}</code>: Initial basic subset, by default, the algorithm creates an initial set of clean observations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; hs93(reg0001)
Dict{Any,Any} with 3 entries:
  &quot;outliers&quot; =&gt; [14, 15, 16, 17, 18, 19, 20, 21]
  &quot;t&quot;        =&gt; -3.59263
  &quot;d&quot;        =&gt; [2.04474, 1.14495, -0.0633255, 0.0632934, -0.354349, -0.766818, -1.06862, -1.47638, -0.7…</code></pre><p><strong>References</strong></p><p>Hadi, Ali S., and Jeffrey S. Simonoff. &quot;Procedures for the identification of  multiple outliers in linear models.&quot; Journal of the American Statistical  Association 88.424 (1993): 1264-1272.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hs93basicsubset-Tuple{RegressionSetting,Array{Int64,1}}" href="#LinRegOutliers.hs93basicsubset-Tuple{RegressionSetting,Array{Int64,1}}"><code>LinRegOutliers.hs93basicsubset</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">hs93basicsubset(setting, initialindices)</code></pre><p>Perform the Hadi &amp; Simonoff (1993) algorithm&#39;s second part for a given regression setting. The returned array of indices are indices of clean subset of length h where h is at least the half of the number of observations. h is set to  integer part of (n + p - 1) / 2.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>initialindices::Array{Int, 1}</code>: (p + 1) subset of clean observations. </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; initialsetindices = hs93initialset(reg0001)
3-element Array{Int64,1}:
 4
 3
 5
 julia&gt; hs93basicsubset(reg0001, initialsetindices)
12-element Array{Int64,1}:
  5
  9
 10
  3
  6
  4
  7
 22
 11
  8
 12
 13</code></pre><p><strong>References</strong></p><p>Hadi, Ali S., and Jeffrey S. Simonoff. &quot;Procedures for the identification of  multiple outliers in linear models.&quot; Journal of the American Statistical  Association 88.424 (1993): 1264-1272.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.hs93initialset-Tuple{RegressionSetting}" href="#LinRegOutliers.hs93initialset-Tuple{RegressionSetting}"><code>LinRegOutliers.hs93initialset</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">hs93initialset(setting)</code></pre><p>Perform the Hadi &amp; Simonoff (1993) algorithm&#39;s first part for a given regression setting. The returned array of indices are indices of clean subset length of p + 1 where p is the number of regression parameters.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; hs93initialset(reg0001)
3-element Array{Int64,1}:
 4
 3
 5</code></pre><p><strong>References</strong></p><p>Hadi, Ali S., and Jeffrey S. Simonoff. &quot;Procedures for the identification of  multiple outliers in linear models.&quot; Journal of the American Statistical  Association 88.424 (1993): 1264-1272.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.imon2005-Tuple{RegressionSetting}" href="#LinRegOutliers.imon2005-Tuple{RegressionSetting}"><code>LinRegOutliers.imon2005</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">imon2005(setting)</code></pre><p>Perform the Imon 2005 algorithm for a given regression setting.</p><p># Arguments </p><ul><li><code>setting::RegressionSetting</code>: A regression setting.</li></ul><p><strong>Notes</strong></p><p>The implementation uses LTS rather than LMS as suggested in the paper. </p><p># Reference A. H. M. Rahmatullah Imon (2005) Identifying multiple influential observations in linear regression,  Journal of Applied Statistics, 32:9, 929-946, DOI: 10.1080/02664760500163599</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.initial_basic_subset_multivariate_data-Tuple{Array{Float64,2},Int64}" href="#LinRegOutliers.initial_basic_subset_multivariate_data-Tuple{Array{Float64,2},Int64}"><code>LinRegOutliers.initial_basic_subset_multivariate_data</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">    initial_basic_subset_multivariate_data(X, m, method=&quot;mahalanobis&quot;)</code></pre><p>This function returns the m-subset according to algorithm #1 for multivariate data. Two methods V1 and V2 are defined in the paper which use Mahalanobis distance or distance from the coordinate-wise median. The m subset returned is guaranteed to be full rank.</p><p><strong>Arguments</strong></p><ul><li><code>X</code>: The multivariate matrix where each row is a data point</li><li><code>m</code>: The number of points to include in the initial subset</li><li><code>method</code>: The distance method to use for selecting the points for initial subset</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.iterateCSteps-Tuple{RegressionSetting,Array{Int64,1},Int64}" href="#LinRegOutliers.iterateCSteps-Tuple{RegressionSetting,Array{Int64,1},Int64}"><code>LinRegOutliers.iterateCSteps</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">iterateCSteps(setting, subsetindices, h)</code></pre><p>Perform a concentration step for a given subset of a regression setting. </p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>subsetindices::Array{Int, 1}</code>: Indicies of observations in the initial subset.</li><li><code>h::Int</code>: A constant at least half of the number of observations.</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">This function is a helper for the lts function. A concentration step starts with a 
initial subset. The size of the subset is enlarged to h, a constant at least half of the 
observations. Please refer to the citation given below.</code></pre><p><strong>References</strong></p><p>Rousseeuw, Peter J., and Katrien Van Driessen. &quot;An algorithm for positive-breakdown  regression based on concentration steps.&quot; Data Analysis.  Springer, Berlin, Heidelberg, 2000. 335-346.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.jacknifedS-Tuple{RegressionSetting,Array{Int64,1}}" href="#LinRegOutliers.jacknifedS-Tuple{RegressionSetting,Array{Int64,1}}"><code>LinRegOutliers.jacknifedS</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">jacknifedS(setting, omittedIndices)</code></pre><p>Calculate Jacknife standard error in which the given indices are omitted from the data.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>omittedIndices::Array{Int, 1}</code>: Indices of omitted variables.</li></ul><p><strong>References</strong></p><p>Peña, Daniel, and Victor J. Yohai. &quot;The detection of influential subsets in linear  regression by using an influence matrix.&quot; Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.jacknifedS-Tuple{RegressionSetting,Int64}" href="#LinRegOutliers.jacknifedS-Tuple{RegressionSetting,Int64}"><code>LinRegOutliers.jacknifedS</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">jacknifedS(setting, k)</code></pre><p>Estimate standard error of regression with the kth observation is dropped.</p><p># Arguments</p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>k::Int</code>: Index of the omitted observation. </li></ul><p># Examples</p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; jacknifedS(reg, 2)
57.518441664761035

julia&gt; jacknifedS(reg, 15)
56.14810222161477</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ks89-Tuple{RegressionSetting}" href="#LinRegOutliers.ks89-Tuple{RegressionSetting}"><code>LinRegOutliers.ks89</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ks89(setting; alpha = 0.05)</code></pre><p>Perform the Kianifard &amp; Swallow (1989) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>alpha::Float64</code>: Optional argument of the probability of rejecting the null hypothesis.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)
julia&gt; ks89(reg0001)
2-element Array{Int64,1}:
  4
 21</code></pre><p><strong>References</strong></p><p>Kianifard, Farid, and William H. Swallow. &quot;Using recursive residuals, calculated on adaptively-ordered observations, to identify outliers in linear regression.&quot; Biometrics (1989): 571-585.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ks89RecursiveResidual-Tuple{RegressionSetting,Array{Int64,1},Int64}" href="#LinRegOutliers.ks89RecursiveResidual-Tuple{RegressionSetting,Array{Int64,1},Int64}"><code>LinRegOutliers.ks89RecursiveResidual</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ks89RecursiveResidual(setting; indices, k)</code></pre><p>Calculate recursive residual for the given regression setting and observation.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>indices::ArrayInt,1</code>: Indices of observations used in the linear model.</li><li><code>k::Int</code>: Observation indice the recursive residual is calculated for.</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">This is a helper function for the ks89 function and it is not directly used.</code></pre><p><strong>References</strong></p><p>Kianifard, Farid, and William H. Swallow. &quot;Using recursive residuals, calculated on adaptively-ordered observations, to identify outliers in linear regression.&quot; Biometrics (1989): 571-585.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lad-Tuple{Array{Float64,2},Array{Float64,1}}" href="#LinRegOutliers.lad-Tuple{Array{Float64,2},Array{Float64,1}}"><code>LinRegOutliers.lad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">lad(X, y; starting_betas = nothing)</code></pre><p>Perform Least Absolute Deviations regression for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array{Float64, 2}</code>: Design matrix of the linear model.</li><li><code>y::Array{Float64, 1}</code>: Response vector of the linear model.</li><li><code>starting_betas::Array{Float64,1}</code>: Starting values of parameter estimations that fed to local search optimizer.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lad-Tuple{RegressionSetting}" href="#LinRegOutliers.lad-Tuple{RegressionSetting}"><code>LinRegOutliers.lad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">lad(setting; starting_betas = nothing)</code></pre><p>Perform Least Absolute Deviations regression for a given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>starting_betas::Array{Float64,1}</code>: Starting values of parameter estimations that fed to local search optimizer.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; lad(reg0001)
Dict{Any,Any} with 2 entries:
  &quot;betas&quot;     =&gt; [-57.3269, 1.19155]
  &quot;residuals&quot; =&gt; [2.14958, 1.25803, 0.0664872, 0.0749413, -0.416605, -0.90815, -1.2997, -1.79124,…
</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lms-Tuple{RegressionSetting}" href="#LinRegOutliers.lms-Tuple{RegressionSetting}"><code>LinRegOutliers.lms</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">lms(setting; iters = nothing, crit = 2.5)</code></pre><p>Perform Least Median of Squares regression estimator with random sampling.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>iters::Int</code>: Number of random samples.</li><li><code>crit::Float64</code>: Critical value for standardized residuals. </li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);

julia&gt; lms(reg)
Dict{Any,Any} with 6 entries:
  &quot;stdres&quot;    =&gt; [2.28328, 1.55551, 0.573308, 0.608843, 0.220321, -0.168202, -0.471913, -0.860435, -0.31603, -0.110871  …  85.7265, 88.9849, 103.269, 116.705, 135.229, 159.69,…
  &quot;S&quot;         =&gt; 1.17908
  &quot;outliers&quot;  =&gt; [14, 15, 16, 17, 18, 19, 20, 21]
  &quot;objective&quot; =&gt; 0.515348
  &quot;coef&quot;      =&gt; [-56.1972, 1.1581]
  &quot;crit&quot;      =&gt; 2.5</code></pre><p><strong>References</strong></p><p>Rousseeuw, Peter J. &quot;Least median of squares regression.&quot; Journal of the American  statistical association 79.388 (1984): 871-880.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lta-Tuple{Array{Float64,2},Array{Float64,1}}" href="#LinRegOutliers.lta-Tuple{Array{Float64,2},Array{Float64,1}}"><code>LinRegOutliers.lta</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">lta(X, y; exact = false)</code></pre><p>Perform the Hawkins &amp; Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array{Float64, 2}</code>: Design matrix of linear regression model.</li><li><code>y::Array{Float64, 1}</code>: Response vector of linear regression model.</li><li><code>exact::Bool</code>: Consider all possible subsets of p or not where p is the number of regression parameters.</li></ul><p><strong>References</strong></p><p>Hawkins, Douglas M., and David Olive. &quot;Applications and algorithms for least trimmed sum of  absolute deviations regression.&quot; Computational Statistics &amp; Data Analysis 32.2 (1999): 119-134.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lta-Tuple{RegressionSetting}" href="#LinRegOutliers.lta-Tuple{RegressionSetting}"><code>LinRegOutliers.lta</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">lta(setting; exact = false)</code></pre><p>Perform the Hawkins &amp; Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>exact::Bool</code>: Consider all possible subsets of p or not where p is the number of regression parameters.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; lta(reg0001)
Dict{Any,Any} with 2 entries:
  &quot;betas&quot;     =&gt; [-55.5, 1.15]
  &quot;objective&quot; =&gt; 5.7

julia&gt; lta(reg0001, exact = true)
Dict{Any,Any} with 2 entries:
  &quot;betas&quot;     =&gt; [-55.5, 1.15]
  &quot;objective&quot; =&gt; 5.7  </code></pre><p><strong>References</strong></p><p>Hawkins, Douglas M., and David Olive. &quot;Applications and algorithms for least trimmed sum of  absolute deviations regression.&quot; Computational Statistics &amp; Data Analysis 32.2 (1999): 119-134.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.lts-Tuple{RegressionSetting}" href="#LinRegOutliers.lts-Tuple{RegressionSetting}"><code>LinRegOutliers.lts</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">lts(setting; iters, crit)</code></pre><p>Perform the Fast-LTS (Least Trimmed Squares) algorithm for a given regression setting. </p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li><li><code>iters::Int</code>: Number of iterations.</li><li><code>crit::Float64</code>: Critical value.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; lts(reg)
Dict{Any,Any} with 6 entries:
  &quot;betas&quot;            =&gt; [-56.5219, 1.16488]
  &quot;S&quot;                =&gt; 1.10918
  &quot;hsubset&quot;          =&gt; [11, 10, 5, 6, 23, 12, 13, 9, 24, 7, 3, 4, 8]
  &quot;outliers&quot;         =&gt; [14, 15, 16, 17, 18, 19, 20, 21]
  &quot;scaled.residuals&quot; =&gt; [2.41447, 1.63472, 0.584504, 0.61617, 0.197052, -0.222066, -0.551027, -0.970146, -0.397538, -0.185558  …  …
  &quot;objective&quot;        =&gt; 3.43133
</code></pre><p><strong>References</strong></p><p>Rousseeuw, Peter J., and Katrien Van Driessen. &quot;An algorithm for positive-breakdown  regression based on concentration steps.&quot; Data Analysis.  Springer, Berlin, Heidelberg, 2000. 335-346.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.mahalanobisBetweenPairs-Tuple{Array{Float64,2}}" href="#LinRegOutliers.mahalanobisBetweenPairs-Tuple{Array{Float64,2}}"><code>LinRegOutliers.mahalanobisBetweenPairs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mahalanobisBetweenPairs(dataMatrix)</code></pre><p>Calculate Mahalanobis distances between pairs. </p><p><strong>Arguments</strong></p><ul><li><code>dataMatrix::Array{Float64, 1}</code>: Data matrix with dimensions n x p, where n is the number of observations and p is the number of variables.</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">Differently from Mahalabonis distances, this function calculates Mahalanobis distances between 
pairs, rather than the distances to center of the data. This is the helper function for the 
dataimage() function defined in Marchette &amp; Solka (2003).</code></pre><p><strong>References</strong></p><p>Marchette, David J., and Jeffrey L. Solka. &quot;Using data images for outlier detection.&quot;  Computational Statistics &amp; Data Analysis 43.4 (2003): 541-552.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.mahalanobisSquaredMatrix-Tuple{DataFrame}" href="#LinRegOutliers.mahalanobisSquaredMatrix-Tuple{DataFrame}"><code>LinRegOutliers.mahalanobisSquaredMatrix</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mahalanobisSquaredMatrix(data::DataFrame; meanvector=nothing, covmatrix=nothing)</code></pre><p>Calculate Mahalanobis distances.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: A DataFrame object of the multivariate data.</li><li><code>meanvector::Array{Float64, 1}</code>: Optional mean vector of variables.</li><li><code>covmatrix::Array{Float64, 2}</code>: Optional covariance matrix of data.</li></ul><p># References Mahalanobis, Prasanta Chandra. &quot;On the generalized distance in statistics.&quot;  National Institute of Science of India, 1936.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.mcd-Tuple{DataFrame}" href="#LinRegOutliers.mcd-Tuple{DataFrame}"><code>LinRegOutliers.mcd</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mcd(data; alpha = 0.01)</code></pre><p>Performs the Minimum Covariance Determinant algorithm for a robust covariance matrix.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Multivariate data.</li><li><code>alpha::Float64</code>: Probability for quantiles of Chi-Squared statistic.</li></ul><p># Notes Algorithm is implemented using concentration steps as described in the reference paper. However, details about number of iterations are slightly different.</p><p><strong>References</strong></p><p>Rousseeuw, Peter J., and Katrien Van Driessen. &quot;A fast algorithm for the minimum covariance  determinant estimator.&quot; Technometrics 41.3 (1999): 212-223.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.midlist-Tuple{Int64,Int64}" href="#LinRegOutliers.midlist-Tuple{Int64,Int64}"><code>LinRegOutliers.midlist</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">midlist(n::Int, p::Int)</code></pre><p>Return p indices in the middle of 1:n.</p><p><strong>Arguments</strong></p><ul><li><code>n::Int</code>: Number of observations.</li><li><code>p::Int</code>: Number of elements in the middle of indices.</li></ul><p><strong>Notes</strong></p><pre><code class="language-none">If n is even and p is odd then p + 1 observation indices are returned.</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; midlist(10,2)
2-element Array{Int64,1}:
 5
 6

julia&gt; midlist(10,3)
4-element Array{Int64,1}:
 4
 5
 6
 7</code></pre><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.mve-Tuple{DataFrame}" href="#LinRegOutliers.mve-Tuple{DataFrame}"><code>LinRegOutliers.mve</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mve(data; alpha = 0.01)</code></pre><p>Performs the Minimum Volume Ellipsoid algorithm for a robust covariance matrix.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: Multivariate data.</li><li><code>alpha::Float64</code>: Probability for quantiles of Chi-Squared statistic.</li></ul><p><strong>References</strong></p><p>Van Aelst, Stefan, and Peter Rousseeuw. &quot;Minimum volume ellipsoid.&quot; Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.mveltsplot-Tuple{RegressionSetting}" href="#LinRegOutliers.mveltsplot-Tuple{RegressionSetting}"><code>LinRegOutliers.mveltsplot</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">mveltsplot(setting; alpha = 0.05, showplot = true)</code></pre><p>Generate MVE - LTS plot for visual detecting of regression outliers.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li><li><code>alpha::Float64</code>: Probability for quantiles of Chi-Squared statistic.</li><li><code>showplot::Bool</code>: Whether a plot is shown or only return statistics.</li></ul><p><strong>References</strong></p><p>Van Aelst, Stefan, and Peter Rousseeuw. &quot;Minimum volume ellipsoid.&quot; Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ndsranks-Tuple{Array{T,2} where T}" href="#LinRegOutliers.ndsranks-Tuple{Array{T,2} where T}"><code>LinRegOutliers.ndsranks</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ndsranks(data)</code></pre><p>Sort multidimensional data usin non-dominated sorting algorithm.</p><p><strong>Arguments</strong></p><ul><li><code>data::Matrix</code>: n x k matrix of observations where n is number of observations and k is number of variables.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; datamat = convert(Matrix, hbk)
75×4 Array{Float64,2}:
 10.1  19.6  28.3   9.7
  9.5  20.5  28.9  10.1
 10.7  20.2  31.0  10.3
  9.9  21.5  31.7   9.5
 10.3  21.1  31.1  10.0
 10.8  20.4  29.2  10.0
 10.5  20.9  29.1  10.8
  9.9  19.6  28.8  10.3
  9.7  20.7  31.0   9.6
  9.3  19.7  30.3   9.9
 11.0  24.0  35.0  -0.2
 12.0  23.0  37.0  -0.4
  ⋮                
  2.8   3.0   2.9  -0.5
  2.0   0.7   2.7   0.6
  0.2   1.8   0.8  -0.9
  1.6   2.0   1.2  -0.7
  0.1   0.0   1.1   0.6
  2.0   0.6   0.3   0.2
  1.0   2.2   2.9   0.7
  2.2   2.5   2.3   0.2
  0.6   2.0   1.5  -0.2
  0.3   1.7   2.2   0.4
  0.0   2.2   1.6  -0.9
  0.3   0.4   2.6   0.2

julia&gt; ndsranks(datamat)
75-element Array{Int64,1}:
 61
 61
 64
 61
 64
 62
 64
 61
 61
 61
 30
 23
  ⋮
 11
  7
  0
  2
  0
  0
 12
 14
  4
  1
  0
  0</code></pre><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p><p>Deb, Kalyanmoy, et al. &quot;A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II.&quot;  International conference on parallel problem solving from nature. Springer, Berlin, Heidelberg, 2000.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ndsranks-Tuple{DataFrame}" href="#LinRegOutliers.ndsranks-Tuple{DataFrame}"><code>LinRegOutliers.ndsranks</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ndsranks(data)</code></pre><p>Sort multidimensional data usin non-dominated sorting algorithm.</p><p><strong>Arguments</strong></p><ul><li><code>data::DataFrame</code>: DataFrame of variables.</li></ul><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p><p>Deb, Kalyanmoy, et al. &quot;A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II.&quot;  International conference on parallel problem solving from nature. Springer, Berlin, Heidelberg, 2000.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.py95-Tuple{RegressionSetting}" href="#LinRegOutliers.py95-Tuple{RegressionSetting}"><code>LinRegOutliers.py95</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">py95(setting)</code></pre><p>Perform the Pena &amp; Yohai (1995) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);
julia&gt; py95(reg0001)
ict{Any,Any} with 2 entries:
  &quot;outliers&quot;       =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]
  &quot;suspected.sets&quot; =&gt; Set([[14, 13], [43, 54, 24, 38, 22], [6, 10], [14, 7, 8, 3, 10, 2, 5, 6, 1, 9, 4…</code></pre><p><strong>References</strong></p><p>Peña, Daniel, and Victor J. Yohai. &quot;The detection of influential subsets in linear  regression by using an influence matrix.&quot; Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.py95ProcessEigenVector-Tuple{Array{Float64,1}}" href="#LinRegOutliers.py95ProcessEigenVector-Tuple{Array{Float64,1}}"><code>LinRegOutliers.py95ProcessEigenVector</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">py95ProcessEigenVector(v)</code></pre><p>Process eigen vectors of EDHDE matrix as defined in the Pena &amp; Yohai (1995) algorithm.</p><p><strong>Arguments</strong></p><ul><li><code>v::Array{Float64, 1}</code>: Eigen vector of EDHDE matrix.</li></ul><p><strong>References</strong></p><p>Peña, Daniel, and Victor J. Yohai. &quot;The detection of influential subsets in linear  regression by using an influence matrix.&quot; Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.py95SuspectedObservations-Tuple{RegressionSetting}" href="#LinRegOutliers.py95SuspectedObservations-Tuple{RegressionSetting}"><code>LinRegOutliers.py95SuspectedObservations</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">py95SuspectedObservations(setting)</code></pre><p>Determine suspected observations (outliers) as defined in the Pena &amp; Yohai (1995) algorithm.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>References</strong></p><p>Peña, Daniel, and Victor J. Yohai. &quot;The detection of influential subsets in linear  regression by using an influence matrix.&quot; Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.ransac-Tuple{RegressionSetting}" href="#LinRegOutliers.ransac-Tuple{RegressionSetting}"><code>LinRegOutliers.ransac</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ransac(setting; t, w=0.5, m=0, k=0, d=0, confidence=0.99)</code></pre><p>Run the RANSAC (1981) algorithm for the given regression setting</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and a dataset.</li><li><code>t::Float64</code>: The threshold distance of a sample point to the regression hyperplane to determine if it fits the model well.</li><li><code>w::Float64</code>: The probability of a sample point being inlier, default=0.5.</li><li><code>m::Int</code>: The number of points to sample to estimate the model parameter for each iteration. If set to 0, defaults to picking p points which is the minimum required.</li><li><code>k::Int</code>: The number of iterations to run. If set to 0, is calculated according to the formula given in the paper based on outlier probability and the sample set size.</li><li><code>d::Int</code>: The number of close data points required to accept the model. Defaults to number of data points multiplied by inlier ratio.</li><li><code>confidence::Float64</code>: Required to determine the number of optimum iterations if k is not specified.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; df = DataFrame(y=[0,1,2,3,3,4,10], x=[0,1,2,2,3,4,2])
julia&gt; reg = createRegressionSetting(@formula(y ~ x), df)
julia&gt; ransac(reg, t=0.8, w=0.85)
1-element Array{Int64,1}:
 7</code></pre><p><strong>References</strong></p><p>Martin A. Fischler &amp; Robert C. Bolles (June 1981). &quot;Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography&quot; Comm. ACM. 24 (6): 381–395.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.responseVector-Tuple{RegressionSetting}" href="#LinRegOutliers.responseVector-Tuple{RegressionSetting}"><code>LinRegOutliers.responseVector</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">responseVector(setting)

Return vector of dependent variable of a given regression setting.</code></pre><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; setting = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; responseVector(setting)
24-element Array{Float64,1}:
   4.4
   4.7
   4.7
   5.9
   6.6
   7.3
   8.1
   8.8
  10.6
  12.0
  13.5
  14.9
  16.1
  21.2
 119.0
 124.0
 142.0
 159.0
 182.0
 212.0
  43.0
  24.0
  27.0
  29.0</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.satman2013-Tuple{RegressionSetting}" href="#LinRegOutliers.satman2013-Tuple{RegressionSetting}"><code>LinRegOutliers.satman2013</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">satman2013(setting)</code></pre><p>Perform Satman (2013) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);
julia&gt; satman2013(reg0001)
Dict{Any,Any} with 1 entry:
  &quot;outliers&quot; =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 47]
</code></pre><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.satman2015-Tuple{RegressionSetting}" href="#LinRegOutliers.satman2015-Tuple{RegressionSetting}"><code>LinRegOutliers.satman2015</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">satman2013(setting)</code></pre><p>Perform Satman (2015) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);
julia&gt; satman2013(reg0001)
Dict{Any,Any} with 1 entry:
  &quot;outliers&quot; =&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 47]
</code></pre><p><strong>References</strong></p><p>Satman, Mehmet Hakan. &quot;A new algorithm for detecting outliers in linear regression.&quot;  International Journal of statistics and Probability 2.3 (2013): 101.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.select_subset-Tuple{Array{Float64,2},Int64,Array{Float64,N} where N}" href="#LinRegOutliers.select_subset-Tuple{Array{Float64,2},Int64,Array{Float64,N} where N}"><code>LinRegOutliers.select_subset</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">    select_subset(X, m, distances)</code></pre><p>This function returns the list of indices which have the least distances as given in the distances array. It also guarantees that at least m indices are returned and that the selected indices have the full rank.</p><p><strong>Arguments</strong></p><ul><li><code>X</code>: The multivariate matrix where each row is a data point.</li><li><code>m</code>: The minimum number of points to include in the subset indices.</li><li><code>distances</code>: The distances vector used for selecting minumum distance indices.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.smr98-Tuple{RegressionSetting}" href="#LinRegOutliers.smr98-Tuple{RegressionSetting}"><code>LinRegOutliers.smr98</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">smr98(setting)</code></pre><p>Perform the Sebert, Monthomery and Rollier (1998) algorithm for the given regression setting.</p><p><strong>Arguments</strong></p><ul><li><code>setting::RegressionSetting</code>: RegressionSetting object with a formula and dataset.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg0001 = createRegressionSetting(@formula(calls ~ year), phones);
julia&gt; smr98(reg0001)
10-element Array{Int64,1}:
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24</code></pre><p><strong>References</strong></p><p>Sebert, David M., Douglas C. Montgomery, and Dwayne A. Rollier. &quot;A clustering algorithm for  identifying multiple outliers in linear regression.&quot; Computational statistics &amp; data analysis  27.4 (1998): 461-484.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="LinRegOutliers.studentizedResiduals-Tuple{RegressionSetting}" href="#LinRegOutliers.studentizedResiduals-Tuple{RegressionSetting}"><code>LinRegOutliers.studentizedResiduals</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">studentizedResiduals(setting)</code></pre><p>Calculate Studentized residuals for a given regression setting.</p><p># Arguments:</p><ul><li><code>setting::RegressionSetting</code>: A regression setting object.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; reg = createRegressionSetting(@formula(calls ~ year), phones);

julia&gt; studentizedResiduals(reg)
24-element Array{Float64,1}:
  0.2398783264505892
  0.1463945666608097
  0.04934549995087145
 -0.023289236798461784
 -0.10408303320973748
 -0.18382934382804111
 -0.2609395640240455
 -0.33934473417314376
 -0.3973205657179429
 -0.46258080183149236
 -0.5261488085924144
 -0.5918396227060093
 -0.6616423337899147
 -0.6611792918262785
  1.0277190922689816
  1.0297863954540103
  1.2712201589839855
  1.4974523565936426
  1.8386296155264197
  2.316394853333409
 -0.9368354141338643
 -1.4009989983319822
 -1.4541520919831887
 -1.529459974327181</code></pre></div></section></article></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 31 October 2020 23:29">Saturday 31 October 2020</span>. Using Julia version 1.5.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
