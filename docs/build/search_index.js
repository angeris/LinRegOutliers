var documenterSearchIndex = {"docs":
[{"location":"#LinRegOutliers.jl-Documentation","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.jl Documentation","text":"","category":"section"},{"location":"","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.jl Documentation","text":"Modules = [LinRegOutliers]\nOrder   = [:type, :function]","category":"page"},{"location":"#LinRegOutliers.RegressionSetting","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.RegressionSetting","text":"struct RegressionSetting\n    formula::FormulaTerm\n    data::DataFrame\nend\n\nImmutable data structure for a regression setting.\n\nArguments\n\nformula::FormulaTerm: A formula object describes the linear regression model.\ndata::DataFrame: DataFrame object holds the data\n\nNotes\n\nImplemented methods in this packages accepts linear models as RegressionSetting objects.\nThis objects holds the model formula and the data used in regression estimations.\n\nExamples\n\njulia> setting = RegressionSetting(@formula(calls ~ year), phones)\nRegressionSetting(calls ~ year, 24×2 DataFrame\n│ Row │ year  │ calls   │\n│     │ Int64 │ Float64 │\n├─────┼───────┼─────────┤\n│ 1   │ 50    │ 4.4     │\n│ 2   │ 51    │ 4.7     │\n│ 3   │ 52    │ 4.7     │\n│ 4   │ 53    │ 5.9     │\n│ 5   │ 54    │ 6.6     │\n│ 6   │ 55    │ 7.3     │\n│ 7   │ 56    │ 8.1     │\n│ 8   │ 57    │ 8.8     │\n│ 9   │ 58    │ 10.6    │\n│ 10  │ 59    │ 12.0    │\n⋮\n│ 14  │ 63    │ 21.2    │\n│ 15  │ 64    │ 119.0   │\n│ 16  │ 65    │ 124.0   │\n│ 17  │ 66    │ 142.0   │\n│ 18  │ 67    │ 159.0   │\n│ 19  │ 68    │ 182.0   │\n│ 20  │ 69    │ 212.0   │\n│ 21  │ 70    │ 43.0    │\n│ 22  │ 71    │ 24.0    │\n│ 23  │ 72    │ 27.0    │\n│ 24  │ 73    │ 29.0    │)\n\n\n\n\n\n","category":"type"},{"location":"#LinRegOutliers.adjustedResiduals-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.adjustedResiduals","text":"adjustedResiduals(setting)\n\nCalculate adjusted residuals for a given regression setting.\n\n# Arguments:\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> adjustedResiduals(reg)\n24-element Array{Float64,1}:\n  13.486773572526268\n   8.2307993473897\n   2.774371467851612\n  -1.3093999279776498\n  -5.851901346871404\n -10.335509559699863\n -14.670907823058053\n -19.07911256736661\n -22.338710565623828\n -26.00786250934617\n -29.58187157605512\n -33.27523207616458\n -37.19977737822219\n -37.173743587631165\n  57.781855070799956\n  57.898085871534626\n  71.47231139524963\n  84.19185329435882\n 103.37399662263209\n 130.23557965295348\n -52.6720662600165\n -78.76891816539992\n -81.75736547266746\n -85.9914301855088\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.applyColumns-Tuple{Function,Array{T,2} where T}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.applyColumns","text":"applyColumns(f, data)\n\nApply function f to each columns of data.\n\nArguments\n\nf::Function: A function that takes a one dimensional array as argument.\ndata::Matrix: A Matrix object.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.applyColumns-Tuple{Function,DataFrame}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.applyColumns","text":"applyColumns(f, data)\n\nApply function f to each columns of data.\n\nArguments\n\nf::Function: A function that takes a one dimensional array as argument.\ndata::DataFrame: A DataFrame object.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.asm2000-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.asm2000","text":"asm2000(setting)\n\nPerform the Setan, Halim and Mohd (2000) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> asm2000(reg0001)\nDict{Any,Any} with 1 entry:\n  \"outliers\" => [15, 16, 17, 18, 19, 20]\n\nReferences\n\nSetan, Halim, and Mohd Nor Mohamad. \"Identifying multiple outliers in  linear regression: Robust fit and clustering approach.\" (2000).\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.atkinson94-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.atkinson94","text":"    atkinson94(setting, iters, crit)\n\nRuns the Atkinson94 algorithm to find out outliers using LMS method.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\niters::Int: Number of random samples.\ncrit::Float64: Critical value for residuals\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)\njulia> atkinson94(reg)\nDict{Any,Any} with 6 entries:\n  \"optimum_index\"    => 10\n  \"residuals_matrix\" => [0.0286208 0.0620609 … 0.0796249 0.0; 0.0397778 0.120547 … 0.118437 0.0397778; … ; 1.21133 1.80846 … 0.690327 4.14366; 1.61977 0.971592 … 0.616204 3.58098]\n  \"outliers\"         => [1, 3, 4, 21]\n  \"objective\"        => 0.799134\n  \"coef\"             => [-38.3133, 0.745659, 0.432794, 0.0104587]\n  \"crit\"             => 3.0\n\n\nReferences\n\nAtkinson, Anthony C. \"Fast very robust methods for the detection of multiple outliers.\" Journal of the American Statistical Association 89.428 (1994): 1329-1339.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.atkinsonstalactiteplot-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.atkinsonstalactiteplot","text":"Runs the atkinson94 algorithm and additionally plots the Stalactite text plot as described in the paper. Works for data with less than 100 points at the moment (since screen width of 80-120 is the standard).\n\nReferences\n\nAtkinson, Anthony C. \"Fast very robust methods for the detection of multiple outliers.\" Journal of the American Statistical Association 89.428 (1994): 1329-1339.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.bacon-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.bacon","text":"    bacon(setting, m, method, alpha)\n\nRun the BACON algorithm to detect outliers on regression data.\n\nArguments:\n\nsetting: RegressionSetting object with a formula and a dataset.\nm: The number of elements to be included in the initial subset.\nmethod: The distance method to use for selecting the points for initial subset\nalpha: The quantile used for cutoff\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)\njulia> bacon_multivariate_outlier_detection(reg, m=12)\n4-element Array{Int64,1}:\n  1\n  3\n  4\n 21\n\n\nReferences\n\nBillor, Nedret, Ali S. Hadi, and Paul F. Velleman. \"BACON: blocked adaptive computationally efficient outlier nominators.\" Computational statistics & data analysis 34.3 (2000): 279-298.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.bacon_multivariate_outlier_detection-Tuple{Array{Float64,2},Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.bacon_multivariate_outlier_detection","text":"    bacon_multivariate_outlier_detection(X, m, method, alpha)\n\nThis function performs the outlier detection for multivariate data according to algorithm #3. This is used by algorithm #4 to compute the initial subset.\n\nArguments\n\nX: The multivariate data matrix.\nm: The minimum number of points to include in the initial subset\nmethod: The distance method to use for selecting the points for initial subset\nalpha: The quantile used for cutoff\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.bacon_regression_initial_subset-Tuple{Array{Float64,2},Array{Float64,N} where N,Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.bacon_regression_initial_subset","text":"    bacon_regression_initial_subset(X, y, m, method, alpha)\n\nThis function computes the initial subset having at least m elements which are likely to be free of outliers used for the BACON algorithm.\n\nArguments:\n\nX: The multivariate data matrix.\ny: The response vector.\nm: The minimum number of points to include in the initial subset\nmethod: The distance method to use for selecting the points for initial subset\nalpha: The quantile used for cutoff\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.bch-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.bch","text":"bch(setting; alpha = 0.05, maxiter = 1000, epsilon = 0.000001)\n\nPerform the Billor & Chatterjee & Hadi (2006) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nalpha::Float64: Optional argument of the probability of rejecting the null hypothesis.\nmaxiter::Int: Maximum number of iterations for calculating iterative weighted least squares estimates.\nepsilon::Float64: Accuracy for determining convergency.\n\nExamples\n\njulia> reg  = createRegressionSetting(@formula(calls ~ year), phones);\njulia> Dict{Any,Any} with 7 entries:\n\"betas\"                               => [-55.9205, 1.15572]\n\"squared.normalized.robust.distances\" => [0.104671, 0.0865052, 0.0700692, 0.0553633, 0.0423875, 0.03…\n\"weights\"                             => [0.00186158, 0.00952088, 0.0787321, 0.0787321, 0.0787321, 0…\n\"outliers\"                            => [1, 14, 15, 16, 17, 18, 19, 20, 21]\n\"squared.normalized.residuals\"        => [5.53742e-5, 2.42977e-5, 2.36066e-6, 2.77706e-6, 1.07985e-7…\n\"residuals\"                           => [2.5348, 1.67908, 0.523367, 0.567651, 0.111936, -0.343779, …\n\"basic.subset\"                        => [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  15, 16, 17, 18, 19, 20, …\n\nReferences\n\nBillor, Nedret, Samprit Chatterjee, and Ali S. Hadi. \"A re-weighted least squares method  for robust regression estimation.\" American journal of mathematical and management sciences 26.3-4 (2006): 229-252.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.bchplot-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.bchplot","text":"bchplot(setting::RegressionSetting; alpha=0.05, maxiter=1000, epsilon=0.00001)\n\nPerform the Billor & Chatterjee & Hadi (2006) algorithm and generates outlier plot  for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nalpha::Float64: Optional argument of the probability of rejecting the null hypothesis.\nmaxiter::Int: Maximum number of iterations for calculating iterative weighted least squares estimates.\nepsilon::Float64: Accuracy for determining convergency.\n\nReferences\n\nBillor, Nedret, Samprit Chatterjee, and Ali S. Hadi. \"A re-weighted least squares method  for robust regression estimation.\" American journal of mathematical and management sciences 26.3-4 (2006): 229-252.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.ccf-Tuple{Array{Float64,2},Array{Float64,1}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.ccf","text":"ccf(X, y; starting_lambdas = nothing)\n\nPerform signed gradient descent for clipped convex functions for a given regression setting.\n\nArguments\n\nX::Array{Float64, 2}: Design matrix of the linear model.\ny::Array{Float64, 1}: Response vector of the linear model.\nstarting_lambdas::Array{Float64,1}: Starting values of weighting parameters used by signed gradient descent.\nalpha::Float64: Loss at which a point is labeled as an outlier. If unspecified, will be chosen as p*mean(residuals.^2), where residuals are OLS residuals.\np::Float64: Points that have squared OLS residual greater than p times the mean squared OLS residual are considered outliers.\nmax_iter::Int64: Maximum number of iterations to run signed gradient descent.\nbeta::Float64: Step size parameter.\ntol::Float64: Tolerance below which convergence is declared.\n\nReferences\n\nBarratt, S., Angeris, G. & Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.ccf-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.ccf","text":"ccf(setting; starting_lambdas = nothing)\n\nPerform signed gradient descent for clipped convex functions for a given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nstarting_lambdas::Array{Float64,1}: Starting values of weighting parameters used by signed gradient descent.\nalpha::Float64: Loss at which a point is labeled as an outlier (points with loss ≥ alpha will be called outliers).\nmax_iter::Int64: Maximum number of iterations to run signed gradient descent.\nbeta::Float64: Step size parameter.\ntol::Float64: Tolerance below which convergence is declared.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> ccf(reg0001)\nDict{Any,Any} with 4 entries:\n  \"betas\"     => [-63.4816, 1.30406]\n  \"outliers\"  => [15, 16, 17, 18, 19, 20]\n  \"lambdas\"   => [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  2.77556e-17, 2.77556e-17, 0…\n  \"residuals\" => [-2.67878, -1.67473, -0.37067, -0.266613, 0.337444, 0.941501, 1.44556, 2.04962, 1…\n\n\nReferences\n\nBarratt, S., Angeris, G. & Boyd, S. Minimizing a sum of clipped convex functions. Optim Lett 14, 2443–2459 (2020). https://doi.org/10.1007/s11590-020-01565-4\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.cga-Tuple{}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.cga","text":"Performs a CGA (Compact Genetic Algorithm) search for minimization of an objective function. In the example below, the objective function is to minimize sum of bits of a binary vector. The search method results the optimum vector of [0, 0, ..., 0] where the objective function is zero.\n\nExamples\n\njulia> function f(x)\n           return sum(x)\n       end\nf (generic function with 1 method)\njulia> cga(chsize = 10, costfunction = f, popsize = 100)\n10-element Array{Bool,1}:\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.cgasample-Tuple{Array{Float64,1}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.cgasample","text":"Generates a binary array of values using a probability vector. Each single element of the probability vector is the probability of bit having  the value of 1. When the probability vector is [1, 1, 1, ..., 1] then the sampled vector is [1.0, 1.0, 1.0, ..., 1.0] whereas it is [0.0, 0.0, 0.0, ..., 0.0] when the probability vector is a vector of zeros. The CGA (compact genetic algorithms) search is started using the  probability vector of [0.5, 0.5, 0.5, ..., 0.5] which produces random vectors of either zeros or ones.\n\nExamples\n\njulia> sample([1, 1, 1, 1, 1])\n5-element Array{Bool,1}:\n 1\n 1\n 1\n 1\n 1\njulia> cgasample(ones(10) * 0.5)\n10-element Array{Bool,1}:\n 1\n 1\n 1\n 1\n 0\n 0\n 0\n 1\n 1\n 0\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.cm97-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.cm97","text":"cm97(setting; maxiter = 1000)\n\nPerform the Chatterjee and Mächler (1997) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nExamples\n\njulia> myreg = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)\njulia> result = cm97(myreg)\nDict{String,Any} with 3 entries:\n  \"betas\"      => [-37.0007, 0.839285, 0.632333, -0.113208]\n  \"iterations\" => 22\n  \"converged\"  => true\n\nReferences\n\nChatterjee, Samprit, and Martin Mächler. \"Robust regression: A weighted least squares approach.\"  Communications in Statistics-Theory and Methods 26.6 (1997): 1381-1394.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.compute_t_distance-Tuple{Array{Float64,2},Array{Float64,N} where N,Array{Int64,N} where N}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.compute_t_distance","text":"    compute_t_distance(X, y, subset)\n\nThis function computes the t distance for each point and returns the distance vector.\n\nArguments:\n\nX: The multivariate data matrix.\ny: The output vector\nsubset: The vector which denotes the points inside the subset, used to scale the residuals accordingly.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.cooks-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.cooks","text":"cooks(setting)\n\nCalculate Cook distances for all observations in a regression setting.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\n\njulia> cooks(reg)\n24-element Array{Float64,1}:\n 0.005344774190779822\n 0.0017088194691033689\n 0.00016624914057962608\n 3.1644452583114795e-5\n 0.0005395058666404081\n 0.0014375008774859539\n 0.0024828140956511258\n 0.0036279720445167277\n 0.004357605989540906\n 0.005288503758364767\n 0.006313578057565415\n 0.0076561205696857254\n 0.009568574875389256\n 0.009970039008782357\n 0.02610396373381051\n 0.029272523880917646\n 0.05091236198400663\n 0.08176555044049343\n 0.14380266904640235\n 0.26721539425047447\n 0.051205153558783356\n 0.13401084683481085\n 0.16860324592350226\n 0.2172819114905912\n\nReferences\n\nCook, R. Dennis. \"Detection of influential observation in linear regression.\"  Technometrics 19.1 (1977): 15-18.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.coordinatwisemedians-Tuple{Array{Float64,2}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.coordinatwisemedians","text":"coordinatwisemedians(datamat)\n\nReturn vector of medians of each column in a matrix.\n\nArguments\n\ndatamat::Array{Float64, 2}: A matrix.\n\nExample\n\njulia> mat = [1.0 2.0; 3.0 4.0; 5.0 6.0]\n3×2 Array{Float64,2}:\n 1.0  2.0\n 3.0  4.0\n 5.0  6.0\n\njulia> coordinatwisemedians(mat)\n2-element Array{Float64,1}:\n 3.0\n 4.0\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.covratio-Tuple{RegressionSetting,Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.covratio","text":"covratio(setting, omittedIndex)\n\nApply covariance ratio diagnostic for a given regression setting and observation index.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\nomittedIndex::Int: Index of the omitted observation.\n\nExample\n\njulia> setting = createRegressionSetting(@formula(calls ~ year), phones);\njulia> covratio(setting, 1)\n1.2945913799871505\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.createRegressionSetting-Tuple{StatsModels.FormulaTerm,DataFrame}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.createRegressionSetting","text":"createRegressionSetting(formula, data)\n\nCreate a regression setting for a given formula and data\n\nArguments\n\nformula::FormulaTerm: A formula object describes the linear regression model.\ndata::DataFrame: DataFrame object holds the data\n\nNotes\n\nImplemented methods in this packages accepts linear models as RegressionSetting objects.\nThis objects holds the model formula and the data used in regression estimations.\ncreateRegressionSetting is a helper function for creating RegressionSetting objects.\n\nExamples\n\njulia> setting = createRegressionSetting(@formula(calls ~ year), phones)\nRegressionSetting(calls ~ year, 24×2 DataFrame\n│ Row │ year  │ calls   │\n│     │ Int64 │ Float64 │\n├─────┼───────┼─────────┤\n│ 1   │ 50    │ 4.4     │\n│ 2   │ 51    │ 4.7     │\n│ 3   │ 52    │ 4.7     │\n│ 4   │ 53    │ 5.9     │\n│ 5   │ 54    │ 6.6     │\n│ 6   │ 55    │ 7.3     │\n│ 7   │ 56    │ 8.1     │\n│ 8   │ 57    │ 8.8     │\n│ 9   │ 58    │ 10.6    │\n│ 10  │ 59    │ 12.0    │\n⋮\n│ 14  │ 63    │ 21.2    │\n│ 15  │ 64    │ 119.0   │\n│ 16  │ 65    │ 124.0   │\n│ 17  │ 66    │ 142.0   │\n│ 18  │ 67    │ 159.0   │\n│ 19  │ 68    │ 182.0   │\n│ 20  │ 69    │ 212.0   │\n│ 21  │ 70    │ 43.0    │\n│ 22  │ 71    │ 24.0    │\n│ 23  │ 72    │ 27.0    │\n│ 24  │ 73    │ 29.0    │)\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.dataimage-Tuple{Array{Float64,2}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.dataimage","text":"dataimage(dataMatrix; distance = :mahalanobis)\n\nGenerate the Marchette & Solka (2003) data image for a given data matrix. \n\nArguments\n\ndataMatrix::Array{Float64, 1}: Data matrix with dimensions n x p, where n is the number of observations and p is the number of variables.\ndistance::Symbol: Optional argument for the distance function.\n\nNotes\n\ndistance is :mahalanobis by default, for the Mahalanobis distances. \nuse \n\n    dataimage(mat, distance = :euclidean)\n\nto use Euclidean distances.\n\nExamples\n\njulia> x1 = hbk[:,\"x1\"];\njulia> x2 = hbk[:,\"x2\"];\njulia> x3 = hbk[:,\"x3\"];\njulia> mat = hcat(x1, x2, x3);\njulia> dataimage(mat)\n\nReferences\n\nMarchette, David J., and Jeffrey L. Solka. \"Using data images for outlier detection.\"  Computational Statistics & Data Analysis 43.4 (2003): 541-552.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.designMatrix-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.designMatrix","text":"designMatrix(setting)\n\nReturn matrix of independent variables including the variable (ones) of the constanst term for a given regression setting.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\n\nNotes\n\nDesign matrix is a matrix which holds values of independent variables on its columns for a given linear regression model.\n\nExamples\n\njulia> setting = createRegressionSetting(@formula(calls ~ year), phones);\njulia> designMatrix(setting)\n24×2 Array{Float64,2}:\n 1.0  50.0\n 1.0  51.0\n 1.0  52.0\n 1.0  53.0\n 1.0  54.0\n 1.0  55.0\n 1.0  56.0\n 1.0  57.0\n 1.0  58.0\n 1.0  59.0\n 1.0  60.0\n 1.0  61.0\n 1.0  62.0\n 1.0  63.0\n 1.0  64.0\n 1.0  65.0\n 1.0  66.0\n 1.0  67.0\n 1.0  68.0\n 1.0  69.0\n 1.0  70.0\n 1.0  71.0\n 1.0  72.0\n 1.0  73.0\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.dfbeta-Tuple{RegressionSetting,Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.dfbeta","text":"dfbeta(setting, omittedIndex)\n\nApply DFBETA diagnostic for a given regression setting and observation index.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\nomittedIndex::Int: Index of the omitted observation.\n\nExample\n\njulia> setting = createRegressionSetting(@formula(calls ~ year), phones);\njulia> dfbeta(setting, 1)\n2-element Array{Float64,1}:\n  9.643915678524024\n -0.14686166007904422\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.dffit-Tuple{RegressionSetting,Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.dffit","text":"dffit(setting, i)\n\nCalculate the effect of the ith observation on the linear regression fit.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\ni::Int: Index of the observation.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> dffit(reg, 1)\n2.3008326745719785\n\njulia> dffit(reg, 15)\n2.7880619386124295\n\njulia> dffit(reg, 16)\n3.1116532421969794\n\njulia> dffit(reg, 17)\n4.367981450347031\n\njulia> dffit(reg, 21)\n-5.81610150322166\n\nReferences\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. Regression diagnostics:  Identifying influential data and sources of collinearity. Vol. 571. John Wiley & Sons, 2005.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.dffit-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.dffit","text":"dffit(setting)\n\nCalculate dffit for all observations.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\n\njulia> dffit(reg)\n24-element Array{Float64,1}:\n   2.3008326745719785\n   1.2189579001467337\n   0.35535667547543426\n  -0.14458523141740898\n  -0.5558346324846752\n  -0.8441316814464983\n  -1.0329184407957257\n  -1.16600692151232\n  -1.2005633711667656\n  -1.2549187193476428\n  -1.3195581500053777\n  -1.42383876236147\n  -1.5917690629803474\n  -1.6582086833534504\n   2.7880619386124295\n   3.1116532421969794\n   4.367981450347031\n   5.927603041427858\n   8.442860517217582\n  12.370243663029527\n  -5.81610150322166\n -10.089153963127842\n -12.10803256546825\n -14.67006851119936\n\nReferences\n\nBelsley, David A., Edwin Kuh, and Roy E. Welsch. Regression diagnostics:  Identifying influential data and sources of collinearity. Vol. 571. John Wiley & Sons, 2005.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.dominates-Tuple{Array,Array}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.dominates","text":"dominates(p1::Array, p2::Array)\n\nReturn true if each element in p1 is not less than the corresponding element in p2 and at least one element in p1 is bigger than the corresponding element in p2.\n\nArguments\n\np1::Array: Numeric array of n elements.\np2::Array: Numeric array of n elements.\n\nExamples\n\njulia> dominates([1,2,3], [1,2,1])\ntrue\n\njulia> dominates([0,0,0,0], [1,0,0,0])\nfalse\n\nReferences\n\nSatman, Mehmet Hakan. \"A new algorithm for detecting outliers in linear regression.\"  International Journal of statistics and Probability 2.3 (2013): 101.\n\nDeb, Kalyanmoy, et al. \"A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II.\"  International conference on parallel problem solving from nature. Springer, Berlin, Heidelberg, 2000.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.euclideanDistances-Tuple{Array{Float64,2}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.euclideanDistances","text":"euclideanDistances(dataMatrix)\n\nCalculate Euclidean distances between pairs. \n\nArguments\n\ndataMatrix::Array{Float64, 1}: Data matrix with dimensions n x p, where n is the number of observations and p is the number of variables.\n\nNotes\n\nThis is the helper function for the dataimage() function defined in Marchette & Solka (2003).\n\nReferences\n\nMarchette, David J., and Jeffrey L. Solka. \"Using data images for outlier detection.\"  Computational Statistics & Data Analysis 43.4 (2003): 541-552.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.find_minimum_nonzero-Tuple{Array{Float64,1}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.find_minimum_nonzero","text":"find_minimum_nonzero(arr)\n\nReturn minimum of numbers greater than zero.\n\nArguments\n\narr::Array{Float64, 1}: A function that takes a one dimensional array as argument.\n\nExample\n\njulia> find_minimum_nonzero([0.0, 0.0, 5.0, 1.0])\n1.0\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.galts-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.galts","text":"galts(setting)\n\nPerform Satman(2012) algorithm for estimating LTS coefficients.\n\nArguments\n\nsetting: A regression setting object.\n\nExamples\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> galts(reg)\nDict{Any,Any} with 3 entries:\n  \"betas\"       => [-56.5219, 1.16488]\n  \"best.subset\" => [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 23, 24]\n  \"objective\"   => 3.43133\n\nReferences\n\nSatman, M. Hakan. \"A genetic algorithm based modification on the lts algorithm for large data sets.\"  Communications in Statistics-Simulation and Computation 41.5 (2012): 644-652.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.hadi1992-Tuple{Array{Float64,2}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.hadi1992","text":"hadi1992(multivariateData)\n\nPerform Hadi (1992) algorithm for a given multivariate data. \n\n# Arguments -multivariate::Array{Float64, 2}: Multivariate data.\n\nExamples\n\njulia> multidata = hcat(hbk.x1, hbk.x2, hbk.x3);\n\njulia> hadi1992(multidata)\nDict{Any,Any} with 3 entries:\n  \"outliers\"              => [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n  \"criticial.chi.squared\" => 7.81473\n  \"rth.robust.distance\"   => 5.04541\n\n# Reference Hadi, Ali S. \"Identifying multiple outliers in multivariate data.\"  Journal of the Royal Statistical Society: Series B (Methodological) 54.3 (1992): 761-771.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.hadi1992_handle_singularity-Tuple{Array{Float64,2}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.hadi1992_handle_singularity","text":"hadi1992_handle_singularity(S)\n\nPerform the sub-algorithm of handling singularity defined in Hadi (1992).\n\n# Arguments \n\nS::Array{Float64, 2}: A covariance matrix.\n\n# Reference Hadi, Ali S. \"Identifying multiple outliers in multivariate data.\"  Journal of the Royal Statistical Society: Series B (Methodological) 54.3 (1992): 761-771.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.hadi1994-Tuple{Array{Float64,2}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.hadi1994","text":"hadi1994(multivariateData)\n\nPerform Hadi (1994) algorithm for a given multivariate data.\n\n# Arguments -multivariate::Array{Float64, 2}: Multivariate data.\n\nExamples\n\njulia> multidata = hcat(hbk.x1, hbk.x2, hbk.x3);\n\njulia> hadi1994(multidata)\nDict{Any,Any} with 3 entries:\n  \"outliers\"              => [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n  \"critical.chi.squared\" => 7.81473\n  \"rth.robust.distance\"   => 5.04541\n\n# Reference Hadi, Ali S. \"A modification of a method for the dedection of outliers in multivariate samples\" Journal of the Royal Statistical Society: Series B (Methodological) 56.2 (1994): 393-396.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.hadimeasure-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.hadimeasure","text":"hadimeasure(setting; c = 2.0)\n\nApply Hadi's regression diagnostic for a given regression setting\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\nc::Float64: Critical value selected between 2.0 - 3.0. The default is 2.0.\n\nExample\n\njulia> setting = createRegressionSetting(@formula(calls ~ year), phones);\njulia> hadimeasure(setting)\n\nReferences\n\nChatterjee, Samprit and Hadi, Ali. Regression Analysis by Example.      5th ed. N.p.: John Wiley & Sons, 2012.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.hatmatrix-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.hatmatrix","text":"hatmatrix(setting)\n\nCalculate Hat matrix of dimensions n x n for a given regression setting with n observations.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\n```julia-repl julia> reg = createRegressionSetting(@formula(calls ~ year), phones); julia> size(hatmatrix(reg))\n\n(24, 24)\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.hs93-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.hs93","text":"hs93(setting; alpha = 0.05, basicsubsetindices = nothing)\n\nPerform the Hadi & Simonoff (1993) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nalpha::Float64: Optional argument of the probability of rejecting the null hypothesis.\nbasicsubsetindices::Array{Int, 1}: Initial basic subset, by default, the algorithm creates an initial set of clean observations.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> hs93(reg0001)\nDict{Any,Any} with 3 entries:\n  \"outliers\" => [14, 15, 16, 17, 18, 19, 20, 21]\n  \"t\"        => -3.59263\n  \"d\"        => [2.04474, 1.14495, -0.0633255, 0.0632934, -0.354349, -0.766818, -1.06862, -1.47638, -0.7…\n\nReferences\n\nHadi, Ali S., and Jeffrey S. Simonoff. \"Procedures for the identification of  multiple outliers in linear models.\" Journal of the American Statistical  Association 88.424 (1993): 1264-1272.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.hs93basicsubset-Tuple{RegressionSetting,Array{Int64,1}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.hs93basicsubset","text":"hs93basicsubset(setting, initialindices)\n\nPerform the Hadi & Simonoff (1993) algorithm's second part for a given regression setting. The returned array of indices are indices of clean subset of length h where h is at least the half of the number of observations. h is set to  integer part of (n + p - 1) / 2.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\ninitialindices::Array{Int, 1}: (p + 1) subset of clean observations. \n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> initialsetindices = hs93initialset(reg0001)\n3-element Array{Int64,1}:\n 4\n 3\n 5\n julia> hs93basicsubset(reg0001, initialsetindices)\n12-element Array{Int64,1}:\n  5\n  9\n 10\n  3\n  6\n  4\n  7\n 22\n 11\n  8\n 12\n 13\n\nReferences\n\nHadi, Ali S., and Jeffrey S. Simonoff. \"Procedures for the identification of  multiple outliers in linear models.\" Journal of the American Statistical  Association 88.424 (1993): 1264-1272.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.hs93initialset-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.hs93initialset","text":"hs93initialset(setting)\n\nPerform the Hadi & Simonoff (1993) algorithm's first part for a given regression setting. The returned array of indices are indices of clean subset length of p + 1 where p is the number of regression parameters.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> hs93initialset(reg0001)\n3-element Array{Int64,1}:\n 4\n 3\n 5\n\nReferences\n\nHadi, Ali S., and Jeffrey S. Simonoff. \"Procedures for the identification of  multiple outliers in linear models.\" Journal of the American Statistical  Association 88.424 (1993): 1264-1272.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.imon2005-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.imon2005","text":"imon2005(setting)\n\nPerform the Imon 2005 algorithm for a given regression setting.\n\n# Arguments \n\nsetting::RegressionSetting: A regression setting.\n\nNotes\n\nThe implementation uses LTS rather than LMS as suggested in the paper. \n\n# Reference A. H. M. Rahmatullah Imon (2005) Identifying multiple influential observations in linear regression,  Journal of Applied Statistics, 32:9, 929-946, DOI: 10.1080/02664760500163599\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.initial_basic_subset_multivariate_data-Tuple{Array{Float64,2},Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.initial_basic_subset_multivariate_data","text":"    initial_basic_subset_multivariate_data(X, m, method=\"mahalanobis\")\n\nThis function returns the m-subset according to algorithm #1 for multivariate data. Two methods V1 and V2 are defined in the paper which use Mahalanobis distance or distance from the coordinate-wise median. The m subset returned is guaranteed to be full rank.\n\nArguments\n\nX: The multivariate matrix where each row is a data point\nm: The number of points to include in the initial subset\nmethod: The distance method to use for selecting the points for initial subset\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.iterateCSteps-Tuple{RegressionSetting,Array{Int64,1},Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.iterateCSteps","text":"iterateCSteps(setting, subsetindices, h)\n\nPerform a concentration step for a given subset of a regression setting. \n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nsubsetindices::Array{Int, 1}: Indicies of observations in the initial subset.\nh::Int: A constant at least half of the number of observations.\n\nNotes\n\nThis function is a helper for the lts function. A concentration step starts with a \ninitial subset. The size of the subset is enlarged to h, a constant at least half of the \nobservations. Please refer to the citation given below.\n\nReferences\n\nRousseeuw, Peter J., and Katrien Van Driessen. \"An algorithm for positive-breakdown  regression based on concentration steps.\" Data Analysis.  Springer, Berlin, Heidelberg, 2000. 335-346.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.jacknifedS-Tuple{RegressionSetting,Array{Int64,1}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.jacknifedS","text":"jacknifedS(setting, omittedIndices)\n\nCalculate Jacknife standard error in which the given indices are omitted from the data.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nomittedIndices::Array{Int, 1}: Indices of omitted variables.\n\nReferences\n\nPeña, Daniel, and Victor J. Yohai. \"The detection of influential subsets in linear  regression by using an influence matrix.\" Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.jacknifedS-Tuple{RegressionSetting,Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.jacknifedS","text":"jacknifedS(setting, k)\n\nEstimate standard error of regression with the kth observation is dropped.\n\n# Arguments\n\nsetting::RegressionSetting: A regression setting object.\nk::Int: Index of the omitted observation. \n\n# Examples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> jacknifedS(reg, 2)\n57.518441664761035\n\njulia> jacknifedS(reg, 15)\n56.14810222161477\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.ks89-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.ks89","text":"ks89(setting; alpha = 0.05)\n\nPerform the Kianifard & Swallow (1989) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nalpha::Float64: Optional argument of the probability of rejecting the null hypothesis.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(stackloss ~ airflow + watertemp + acidcond), stackloss)\njulia> ks89(reg0001)\n2-element Array{Int64,1}:\n  4\n 21\n\nReferences\n\nKianifard, Farid, and William H. Swallow. \"Using recursive residuals, calculated on adaptively-ordered observations, to identify outliers in linear regression.\" Biometrics (1989): 571-585.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.ks89RecursiveResidual-Tuple{RegressionSetting,Array{Int64,1},Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.ks89RecursiveResidual","text":"ks89RecursiveResidual(setting; indices, k)\n\nCalculate recursive residual for the given regression setting and observation.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nindices::ArrayInt,1: Indices of observations used in the linear model.\nk::Int: Observation indice the recursive residual is calculated for.\n\nNotes\n\nThis is a helper function for the ks89 function and it is not directly used.\n\nReferences\n\nKianifard, Farid, and William H. Swallow. \"Using recursive residuals, calculated on adaptively-ordered observations, to identify outliers in linear regression.\" Biometrics (1989): 571-585.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.lad-Tuple{Array{Float64,2},Array{Float64,1}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.lad","text":"lad(X, y; starting_betas = nothing)\n\nPerform Least Absolute Deviations regression for a given regression setting.\n\nArguments\n\nX::Array{Float64, 2}: Design matrix of the linear model.\ny::Array{Float64, 1}: Response vector of the linear model.\nstarting_betas::Array{Float64,1}: Starting values of parameter estimations that fed to local search optimizer.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.lad-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.lad","text":"lad(setting; starting_betas = nothing)\n\nPerform Least Absolute Deviations regression for a given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nstarting_betas::Array{Float64,1}: Starting values of parameter estimations that fed to local search optimizer.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> lad(reg0001)\nDict{Any,Any} with 2 entries:\n  \"betas\"     => [-57.3269, 1.19155]\n  \"residuals\" => [2.14958, 1.25803, 0.0664872, 0.0749413, -0.416605, -0.90815, -1.2997, -1.79124,…\n\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.lms-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.lms","text":"lms(setting; iters = nothing, crit = 2.5)\n\nPerform Least Median of Squares regression estimator with random sampling.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\niters::Int: Number of random samples.\ncrit::Float64: Critical value for standardized residuals. \n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\n\njulia> lms(reg)\nDict{Any,Any} with 6 entries:\n  \"stdres\"    => [2.28328, 1.55551, 0.573308, 0.608843, 0.220321, -0.168202, -0.471913, -0.860435, -0.31603, -0.110871  …  85.7265, 88.9849, 103.269, 116.705, 135.229, 159.69,…\n  \"S\"         => 1.17908\n  \"outliers\"  => [14, 15, 16, 17, 18, 19, 20, 21]\n  \"objective\" => 0.515348\n  \"coef\"      => [-56.1972, 1.1581]\n  \"crit\"      => 2.5\n\nReferences\n\nRousseeuw, Peter J. \"Least median of squares regression.\" Journal of the American  statistical association 79.388 (1984): 871-880.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.lta-Tuple{Array{Float64,2},Array{Float64,1}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.lta","text":"lta(X, y; exact = false)\n\nPerform the Hawkins & Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.\n\nArguments\n\nX::Array{Float64, 2}: Design matrix of linear regression model.\ny::Array{Float64, 1}: Response vector of linear regression model.\nexact::Bool: Consider all possible subsets of p or not where p is the number of regression parameters.\n\nReferences\n\nHawkins, Douglas M., and David Olive. \"Applications and algorithms for least trimmed sum of  absolute deviations regression.\" Computational Statistics & Data Analysis 32.2 (1999): 119-134.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.lta-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.lta","text":"lta(setting; exact = false)\n\nPerform the Hawkins & Olive (1999) algorithm (Least Trimmed Absolute Deviations)  for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\nexact::Bool: Consider all possible subsets of p or not where p is the number of regression parameters.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> lta(reg0001)\nDict{Any,Any} with 2 entries:\n  \"betas\"     => [-55.5, 1.15]\n  \"objective\" => 5.7\n\njulia> lta(reg0001, exact = true)\nDict{Any,Any} with 2 entries:\n  \"betas\"     => [-55.5, 1.15]\n  \"objective\" => 5.7  \n\nReferences\n\nHawkins, Douglas M., and David Olive. \"Applications and algorithms for least trimmed sum of  absolute deviations regression.\" Computational Statistics & Data Analysis 32.2 (1999): 119-134.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.lts-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.lts","text":"lts(setting; iters, crit)\n\nPerform the Fast-LTS (Least Trimmed Squares) algorithm for a given regression setting. \n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\niters::Int: Number of iterations.\ncrit::Float64: Critical value.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\njulia> lts(reg)\nDict{Any,Any} with 6 entries:\n  \"betas\"            => [-56.5219, 1.16488]\n  \"S\"                => 1.10918\n  \"hsubset\"          => [11, 10, 5, 6, 23, 12, 13, 9, 24, 7, 3, 4, 8]\n  \"outliers\"         => [14, 15, 16, 17, 18, 19, 20, 21]\n  \"scaled.residuals\" => [2.41447, 1.63472, 0.584504, 0.61617, 0.197052, -0.222066, -0.551027, -0.970146, -0.397538, -0.185558  …  …\n  \"objective\"        => 3.43133\n\n\nReferences\n\nRousseeuw, Peter J., and Katrien Van Driessen. \"An algorithm for positive-breakdown  regression based on concentration steps.\" Data Analysis.  Springer, Berlin, Heidelberg, 2000. 335-346.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.mahalanobisBetweenPairs-Tuple{Array{Float64,2}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.mahalanobisBetweenPairs","text":"mahalanobisBetweenPairs(dataMatrix)\n\nCalculate Mahalanobis distances between pairs. \n\nArguments\n\ndataMatrix::Array{Float64, 1}: Data matrix with dimensions n x p, where n is the number of observations and p is the number of variables.\n\nNotes\n\nDifferently from Mahalabonis distances, this function calculates Mahalanobis distances between \npairs, rather than the distances to center of the data. This is the helper function for the \ndataimage() function defined in Marchette & Solka (2003).\n\nReferences\n\nMarchette, David J., and Jeffrey L. Solka. \"Using data images for outlier detection.\"  Computational Statistics & Data Analysis 43.4 (2003): 541-552.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.mahalanobisSquaredMatrix-Tuple{DataFrame}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.mahalanobisSquaredMatrix","text":"mahalanobisSquaredMatrix(data::DataFrame; meanvector=nothing, covmatrix=nothing)\n\nCalculate Mahalanobis distances.\n\nArguments\n\ndata::DataFrame: A DataFrame object of the multivariate data.\nmeanvector::Array{Float64, 1}: Optional mean vector of variables.\ncovmatrix::Array{Float64, 2}: Optional covariance matrix of data.\n\n# References Mahalanobis, Prasanta Chandra. \"On the generalized distance in statistics.\"  National Institute of Science of India, 1936.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.mcd-Tuple{DataFrame}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.mcd","text":"mcd(data; alpha = 0.01)\n\nPerforms the Minimum Covariance Determinant algorithm for a robust covariance matrix.\n\nArguments\n\ndata::DataFrame: Multivariate data.\nalpha::Float64: Probability for quantiles of Chi-Squared statistic.\n\n# Notes Algorithm is implemented using concentration steps as described in the reference paper. However, details about number of iterations are slightly different.\n\nReferences\n\nRousseeuw, Peter J., and Katrien Van Driessen. \"A fast algorithm for the minimum covariance  determinant estimator.\" Technometrics 41.3 (1999): 212-223.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.midlist-Tuple{Int64,Int64}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.midlist","text":"midlist(n::Int, p::Int)\n\nReturn p indices in the middle of 1:n.\n\nArguments\n\nn::Int: Number of observations.\np::Int: Number of elements in the middle of indices.\n\nNotes\n\nIf n is even and p is odd then p + 1 observation indices are returned.\n\nExamples\n\njulia> midlist(10,2)\n2-element Array{Int64,1}:\n 5\n 6\n\njulia> midlist(10,3)\n4-element Array{Int64,1}:\n 4\n 5\n 6\n 7\n\nReferences\n\nSatman, Mehmet Hakan. \"A new algorithm for detecting outliers in linear regression.\"  International Journal of statistics and Probability 2.3 (2013): 101.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.mve-Tuple{DataFrame}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.mve","text":"mve(data; alpha = 0.01)\n\nPerforms the Minimum Volume Ellipsoid algorithm for a robust covariance matrix.\n\nArguments\n\ndata::DataFrame: Multivariate data.\nalpha::Float64: Probability for quantiles of Chi-Squared statistic.\n\nReferences\n\nVan Aelst, Stefan, and Peter Rousseeuw. \"Minimum volume ellipsoid.\" Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.mveltsplot-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.mveltsplot","text":"mveltsplot(setting; alpha = 0.05, showplot = true)\n\nGenerate MVE - LTS plot for visual detecting of regression outliers.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\nalpha::Float64: Probability for quantiles of Chi-Squared statistic.\nshowplot::Bool: Whether a plot is shown or only return statistics.\n\nReferences\n\nVan Aelst, Stefan, and Peter Rousseeuw. \"Minimum volume ellipsoid.\" Wiley  Interdisciplinary Reviews: Computational Statistics 1.1 (2009): 71-82.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.ndsranks-Tuple{Array{T,2} where T}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.ndsranks","text":"ndsranks(data)\n\nSort multidimensional data usin non-dominated sorting algorithm.\n\nArguments\n\ndata::Matrix: n x k matrix of observations where n is number of observations and k is number of variables.\n\nExamples\n\njulia> datamat = convert(Matrix, hbk)\n75×4 Array{Float64,2}:\n 10.1  19.6  28.3   9.7\n  9.5  20.5  28.9  10.1\n 10.7  20.2  31.0  10.3\n  9.9  21.5  31.7   9.5\n 10.3  21.1  31.1  10.0\n 10.8  20.4  29.2  10.0\n 10.5  20.9  29.1  10.8\n  9.9  19.6  28.8  10.3\n  9.7  20.7  31.0   9.6\n  9.3  19.7  30.3   9.9\n 11.0  24.0  35.0  -0.2\n 12.0  23.0  37.0  -0.4\n  ⋮                \n  2.8   3.0   2.9  -0.5\n  2.0   0.7   2.7   0.6\n  0.2   1.8   0.8  -0.9\n  1.6   2.0   1.2  -0.7\n  0.1   0.0   1.1   0.6\n  2.0   0.6   0.3   0.2\n  1.0   2.2   2.9   0.7\n  2.2   2.5   2.3   0.2\n  0.6   2.0   1.5  -0.2\n  0.3   1.7   2.2   0.4\n  0.0   2.2   1.6  -0.9\n  0.3   0.4   2.6   0.2\n\njulia> ndsranks(datamat)\n75-element Array{Int64,1}:\n 61\n 61\n 64\n 61\n 64\n 62\n 64\n 61\n 61\n 61\n 30\n 23\n  ⋮\n 11\n  7\n  0\n  2\n  0\n  0\n 12\n 14\n  4\n  1\n  0\n  0\n\nReferences\n\nSatman, Mehmet Hakan. \"A new algorithm for detecting outliers in linear regression.\"  International Journal of statistics and Probability 2.3 (2013): 101.\n\nDeb, Kalyanmoy, et al. \"A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II.\"  International conference on parallel problem solving from nature. Springer, Berlin, Heidelberg, 2000.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.ndsranks-Tuple{DataFrame}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.ndsranks","text":"ndsranks(data)\n\nSort multidimensional data usin non-dominated sorting algorithm.\n\nArguments\n\ndata::DataFrame: DataFrame of variables.\n\nReferences\n\nSatman, Mehmet Hakan. \"A new algorithm for detecting outliers in linear regression.\"  International Journal of statistics and Probability 2.3 (2013): 101.\n\nDeb, Kalyanmoy, et al. \"A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II.\"  International conference on parallel problem solving from nature. Springer, Berlin, Heidelberg, 2000.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.py95-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.py95","text":"py95(setting)\n\nPerform the Pena & Yohai (1995) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);\njulia> py95(reg0001)\nict{Any,Any} with 2 entries:\n  \"outliers\"       => [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n  \"suspected.sets\" => Set([[14, 13], [43, 54, 24, 38, 22], [6, 10], [14, 7, 8, 3, 10, 2, 5, 6, 1, 9, 4…\n\nReferences\n\nPeña, Daniel, and Victor J. Yohai. \"The detection of influential subsets in linear  regression by using an influence matrix.\" Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.py95ProcessEigenVector-Tuple{Array{Float64,1}}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.py95ProcessEigenVector","text":"py95ProcessEigenVector(v)\n\nProcess eigen vectors of EDHDE matrix as defined in the Pena & Yohai (1995) algorithm.\n\nArguments\n\nv::Array{Float64, 1}: Eigen vector of EDHDE matrix.\n\nReferences\n\nPeña, Daniel, and Victor J. Yohai. \"The detection of influential subsets in linear  regression by using an influence matrix.\" Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.py95SuspectedObservations-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.py95SuspectedObservations","text":"py95SuspectedObservations(setting)\n\nDetermine suspected observations (outliers) as defined in the Pena & Yohai (1995) algorithm.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nReferences\n\nPeña, Daniel, and Victor J. Yohai. \"The detection of influential subsets in linear  regression by using an influence matrix.\" Journal of the Royal Statistical Society:  Series B (Methodological) 57.1 (1995): 145-156.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.ransac-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.ransac","text":"ransac(setting; t, w=0.5, m=0, k=0, d=0, confidence=0.99)\n\nRun the RANSAC (1981) algorithm for the given regression setting\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and a dataset.\nt::Float64: The threshold distance of a sample point to the regression hyperplane to determine if it fits the model well.\nw::Float64: The probability of a sample point being inlier, default=0.5.\nm::Int: The number of points to sample to estimate the model parameter for each iteration. If set to 0, defaults to picking p points which is the minimum required.\nk::Int: The number of iterations to run. If set to 0, is calculated according to the formula given in the paper based on outlier probability and the sample set size.\nd::Int: The number of close data points required to accept the model. Defaults to number of data points multiplied by inlier ratio.\nconfidence::Float64: Required to determine the number of optimum iterations if k is not specified.\n\nExamples\n\njulia> df = DataFrame(y=[0,1,2,3,3,4,10], x=[0,1,2,2,3,4,2])\njulia> reg = createRegressionSetting(@formula(y ~ x), df)\njulia> ransac(reg, t=0.8, w=0.85)\n1-element Array{Int64,1}:\n 7\n\nReferences\n\nMartin A. Fischler & Robert C. Bolles (June 1981). \"Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography\" Comm. ACM. 24 (6): 381–395.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.responseVector-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.responseVector","text":"responseVector(setting)\n\nReturn vector of dependent variable of a given regression setting.\n\nArguments\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> setting = createRegressionSetting(@formula(calls ~ year), phones);\njulia> responseVector(setting)\n24-element Array{Float64,1}:\n   4.4\n   4.7\n   4.7\n   5.9\n   6.6\n   7.3\n   8.1\n   8.8\n  10.6\n  12.0\n  13.5\n  14.9\n  16.1\n  21.2\n 119.0\n 124.0\n 142.0\n 159.0\n 182.0\n 212.0\n  43.0\n  24.0\n  27.0\n  29.0\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.satman2013-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.satman2013","text":"satman2013(setting)\n\nPerform Satman (2013) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nExamples\n\njulia> eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);\njulia> satman2013(reg0001)\nDict{Any,Any} with 1 entry:\n  \"outliers\" => [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 47]\n\n\nReferences\n\nSatman, Mehmet Hakan. \"A new algorithm for detecting outliers in linear regression.\"  International Journal of statistics and Probability 2.3 (2013): 101.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.satman2015-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.satman2015","text":"satman2013(setting)\n\nPerform Satman (2015) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nExamples\n\njulia> eg0001 = createRegressionSetting(@formula(y ~ x1 + x2 + x3), hbk);\njulia> satman2013(reg0001)\nDict{Any,Any} with 1 entry:\n  \"outliers\" => [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 14, 47]\n\n\nReferences\n\nSatman, Mehmet Hakan. \"A new algorithm for detecting outliers in linear regression.\"  International Journal of statistics and Probability 2.3 (2013): 101.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.select_subset-Tuple{Array{Float64,2},Int64,Array{Float64,N} where N}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.select_subset","text":"    select_subset(X, m, distances)\n\nThis function returns the list of indices which have the least distances as given in the distances array. It also guarantees that at least m indices are returned and that the selected indices have the full rank.\n\nArguments\n\nX: The multivariate matrix where each row is a data point.\nm: The minimum number of points to include in the subset indices.\ndistances: The distances vector used for selecting minumum distance indices.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.smr98-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.smr98","text":"smr98(setting)\n\nPerform the Sebert, Monthomery and Rollier (1998) algorithm for the given regression setting.\n\nArguments\n\nsetting::RegressionSetting: RegressionSetting object with a formula and dataset.\n\nExamples\n\njulia> reg0001 = createRegressionSetting(@formula(calls ~ year), phones);\njulia> smr98(reg0001)\n10-element Array{Int64,1}:\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n\nReferences\n\nSebert, David M., Douglas C. Montgomery, and Dwayne A. Rollier. \"A clustering algorithm for  identifying multiple outliers in linear regression.\" Computational statistics & data analysis  27.4 (1998): 461-484.\n\n\n\n\n\n","category":"method"},{"location":"#LinRegOutliers.studentizedResiduals-Tuple{RegressionSetting}","page":"LinRegOutliers.jl Documentation","title":"LinRegOutliers.studentizedResiduals","text":"studentizedResiduals(setting)\n\nCalculate Studentized residuals for a given regression setting.\n\n# Arguments:\n\nsetting::RegressionSetting: A regression setting object.\n\nExamples\n\njulia> reg = createRegressionSetting(@formula(calls ~ year), phones);\n\njulia> studentizedResiduals(reg)\n24-element Array{Float64,1}:\n  0.2398783264505892\n  0.1463945666608097\n  0.04934549995087145\n -0.023289236798461784\n -0.10408303320973748\n -0.18382934382804111\n -0.2609395640240455\n -0.33934473417314376\n -0.3973205657179429\n -0.46258080183149236\n -0.5261488085924144\n -0.5918396227060093\n -0.6616423337899147\n -0.6611792918262785\n  1.0277190922689816\n  1.0297863954540103\n  1.2712201589839855\n  1.4974523565936426\n  1.8386296155264197\n  2.316394853333409\n -0.9368354141338643\n -1.4009989983319822\n -1.4541520919831887\n -1.529459974327181\n\n\n\n\n\n","category":"method"}]
}
